{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clean-text in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clean-text) (1.7.0)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clean-text) (6.3.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: inflect in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (7.4.0)\n",
      "Requirement already satisfied: more-itertools>=8.5.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from inflect) (10.5.0)\n",
      "Requirement already satisfied: typeguard>=4.0.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from inflect) (4.4.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typeguard>=4.0.1->inflect) (4.12.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install clean-text\n",
    "!pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import inflect\n",
    "from cleantext import clean\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: PIZZA_dev.json\n",
      "Lines: 30\n"
     ]
    }
   ],
   "source": [
    "datatype = 'dev' \n",
    "\n",
    "path = (\n",
    "    'PIZZA_train.json' if datatype == 'train' else\n",
    "    'PIZZA_dev.json' if datatype == 'dev' else\n",
    "    'PIZZA_test.json' if datatype == 'test' else\n",
    "    'unknown.json'  # Optional fallback\n",
    ")\n",
    "\n",
    "lines = (\n",
    "    20 if datatype == 'train' else\n",
    "    30 if datatype == 'dev' else\n",
    "    1357 if datatype == 'test' else\n",
    "    0  \n",
    ")\n",
    "\n",
    "print(f\"Path: {path}\")\n",
    "print(f\"Lines: {lines}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#! load data given file path and number of line to load\n",
    "def load_data(file_path,num_lines):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file: \n",
    "        for _ in range(num_lines):\n",
    "            data.append(json.loads(file.readline())) \n",
    "    return data\n",
    "def load_random_data(file_path, num_lines, last_lines):\n",
    "    data = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        file.seek(0, 2)  # Move to the end of the file to get its size\n",
    "        file_size = file.tell()\n",
    "        \n",
    "        # Read the last `last_lines` lines\n",
    "        file.seek(max(file_size - 1024 * last_lines, 0))  # Optional: Adjust the seek window size for efficiency\n",
    "        lines = file.readlines()\n",
    "        last_lines_data = [json.loads(line.strip()) for line in lines[-last_lines:] if line.strip()]\n",
    "        data.extend(last_lines_data)\n",
    "\n",
    "        # Read `num_lines` random lines from the rest of the file (excluding the last `last_lines` lines)\n",
    "        remaining_lines = file_size - sum(len(line) for line in lines[-last_lines:])\n",
    "        for _ in range(num_lines):\n",
    "            while True:\n",
    "                # Pick a random position outside the last `last_lines` range\n",
    "                random_pos = random.randint(0, remaining_lines - 1)\n",
    "                file.seek(random_pos)\n",
    "                \n",
    "                # Read to the end of the current line to avoid partial lines\n",
    "                file.readline()\n",
    "                line = file.readline()  # Read the next line (complete line)\n",
    "                if line:  # Ensure it's not an empty line\n",
    "                    # Check if it's not from the last `last_lines` lines\n",
    "                    # if any(line.strip() == last_line for last_line in last_lines_data):\n",
    "                    #     continue  # Skip if it's from the last lines\n",
    "                    try:\n",
    "                        data.append(json.loads(line.strip()))\n",
    "                        break  # Exit the loop for this line\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue  # Skip if it's not valid JSON\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dev.SRC': 'i want to order two medium pizzas with sausage and black olives and two medium pizzas with pepperoni and extra cheese and three large pizzas with pepperoni and sausage', 'dev.EXR': '(ORDER (PIZZAORDER (NUMBER 2 ) (SIZE MEDIUM ) (COMPLEX_TOPPING (QUANTITY EXTRA ) (TOPPING CHEESE ) ) (TOPPING PEPPERONI ) ) (PIZZAORDER (NUMBER 2 ) (SIZE MEDIUM ) (TOPPING OLIVES ) (TOPPING SAUSAGE ) ) (PIZZAORDER (NUMBER 3 ) (SIZE LARGE ) (TOPPING PEPPERONI ) (TOPPING SAUSAGE ) ) )', 'dev.TOP': '(ORDER i want to order (PIZZAORDER (NUMBER two ) (SIZE medium ) pizzas with (TOPPING sausage ) and (TOPPING black olives ) ) and (PIZZAORDER (NUMBER two ) (SIZE medium ) pizzas with (TOPPING pepperoni ) and (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING cheese ) ) ) and (PIZZAORDER (NUMBER three ) (SIZE large ) pizzas with (TOPPING pepperoni ) and (TOPPING sausage ) ) )', 'dev.PCFG_ERR': 'False'}, {'dev.SRC': 'five medium pizzas with tomatoes and ham', 'dev.EXR': '(ORDER (PIZZAORDER (NUMBER 5 ) (SIZE MEDIUM ) (TOPPING HAM ) (TOPPING TOMATOES ) ) )', 'dev.TOP': '(ORDER (PIZZAORDER (NUMBER five ) (SIZE medium ) pizzas with (TOPPING tomatoes ) and (TOPPING ham ) ) )', 'dev.PCFG_ERR': 'False'}, {'dev.SRC': 'i need to order one large vegetarian pizza with extra banana peppers', 'dev.EXR': '(ORDER (PIZZAORDER (NUMBER 1 ) (SIZE LARGE ) (STYLE VEGETARIAN ) (COMPLEX_TOPPING (QUANTITY EXTRA ) (TOPPING BANANA_PEPPERS ) ) ) )', 'dev.TOP': '(ORDER i need to order (PIZZAORDER (NUMBER one ) (SIZE large ) (STYLE vegetarian ) pizza with (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING banana peppers ) ) ) )', 'dev.PCFG_ERR': 'False'}]\n"
     ]
    }
   ],
   "source": [
    "data=load_data(path,lines) if datatype=='train' else load_data(path,lines)\n",
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i want to order two medium pizzas with sausage and black olives and two medium pizzas with pepperoni and extra cheese and three large pizzas with pepperoni and sausage', 'five medium pizzas with tomatoes and ham']\n",
      "['(ORDER (PIZZAORDER (NUMBER 2 ) (SIZE MEDIUM ) (COMPLEX_TOPPING (QUANTITY EXTRA ) (TOPPING CHEESE ) ) (TOPPING PEPPERONI ) ) (PIZZAORDER (NUMBER 2 ) (SIZE MEDIUM ) (TOPPING OLIVES ) (TOPPING SAUSAGE ) ) (PIZZAORDER (NUMBER 3 ) (SIZE LARGE ) (TOPPING PEPPERONI ) (TOPPING SAUSAGE ) ) )', '(ORDER (PIZZAORDER (NUMBER 5 ) (SIZE MEDIUM ) (TOPPING HAM ) (TOPPING TOMATOES ) ) )']\n",
      "['(ORDER i want to order (PIZZAORDER (NUMBER two ) (SIZE medium ) pizzas with (TOPPING sausage ) and (TOPPING black olives ) ) and (PIZZAORDER (NUMBER two ) (SIZE medium ) pizzas with (TOPPING pepperoni ) and (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING cheese ) ) ) and (PIZZAORDER (NUMBER three ) (SIZE large ) pizzas with (TOPPING pepperoni ) and (TOPPING sausage ) ) )', '(ORDER (PIZZAORDER (NUMBER five ) (SIZE medium ) pizzas with (TOPPING tomatoes ) and (TOPPING ham ) ) )']\n"
     ]
    }
   ],
   "source": [
    "#! split json data\n",
    "def getextensions(datatype):\n",
    "    if datatype=='train':\n",
    "        return 'train.SRC','train.EXR','train.TOP'\n",
    "    elif datatype=='dev':\n",
    "        return 'dev.SRC','dev.EXR','dev.TOP'\n",
    "    else:\n",
    "        return 'test.SRC','test.EXR','test.TOP'\n",
    "def get_training_data(data,datatype='train'):\n",
    "    values=getextensions(datatype)\n",
    "    training_data = []\n",
    "    training_exr=[]\n",
    "    training_top=[]\n",
    "    # training_top_dec=[]\n",
    "    for item in data:\n",
    "        training_data.append(item[values[0]])  \n",
    "        training_exr.append(item[values[1]])  \n",
    "        training_top.append(item[values[2]])\n",
    "        # if datatype=='train':\n",
    "        #     training_top_dec.append(item[values[3]])  \n",
    "    return training_data,training_exr,training_top\n",
    "training_data,training_exr,training_top=get_training_data(data,datatype)   \n",
    "print(training_data[:2])\n",
    "print(training_exr[:2])\n",
    "print(training_top[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an inflect engine\n",
    "p = inflect.engine()\n",
    "\n",
    "def replace_numbers_with_words(text):\n",
    "    # Regular expression to find numbers in the text\n",
    "    number_pattern = r'\\b\\d+\\b'\n",
    "    \n",
    "    # Function to convert a number match to its word representation\n",
    "    def number_to_words(match):\n",
    "        number = int(match.group())\n",
    "        # Convert to words and replace hyphens with underscores\n",
    "        return p.number_to_words(number, andword=\"\").replace('-', '_')\n",
    "\n",
    "    \n",
    "    # Function to convert a number match to its word representation\n",
    "    def number_to_words(match):\n",
    "        number = int(match.group())\n",
    "        return p.number_to_words(number, andword=\"\").replace('-', '_')\n",
    "    \n",
    "    # Substitute numbers with their word representation\n",
    "    text = re.sub(number_pattern, number_to_words, text)\n",
    "    \n",
    "    # Replace the standalone 'a' with 'one' only when it reprsents a\n",
    "    text = re.sub(\n",
    "        r'\\b[aA]\\b(?! ((lot of)|(little)|(few)|(number of)|(couple of)|(number of)|(bit of)|(number of)|(bit)|(load of)|(stack of)|(bunch of)|(group of)|(set of)|(series of)|(variety of)|(range of)|(number of)|(amount of)|(sum of)|(total)))',\n",
    "        'one',\n",
    "        text\n",
    "    )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! a function to handle negations\n",
    "def handle_negations(text):\n",
    "    #! negations are based on training set\n",
    "    negations_pattern = r\"\\b(?:no|not|without)\\s+.*?\\b(?=(?:[^\\w\\s]|$))\"\n",
    "    # print(re.findall(negations_pattern, text))\n",
    "    text = re.sub(negations_pattern, lambda x: ' '.join([f'not_{word}' for word in x.group(0).split()]), text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!# lemmatize words with all possible pos tags\n",
    "def lem_word(word):\n",
    "    possible_pos = [wordnet.NOUN, wordnet.VERB, wordnet.ADJ, wordnet.ADV]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for pos in possible_pos:\n",
    "        word=lemmatizer.lemmatize(word,pos)\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "#! stopwords list (adding not_)\n",
    "stopwords = set(stopwords.words('english'))\n",
    "stopwords.add('like')\n",
    "not_stopwords = ['not_' + word for word in stopwords]\n",
    "stopwords.update(not_stopwords)\n",
    "# stopwords.discard('a')\n",
    "# stopwords.discard('an')\n",
    "# stopwords.discard('not')\n",
    "# stopwords.discard('no')\n",
    "# stopwords.discard('can')\n",
    "# stopwords.discard('not_a')\n",
    "# stopwords.discard('not_an')\n",
    "# stopwords.discard('not_can')\n",
    "# stopwords.discard('not_no')\n",
    "stopwords=set()\n",
    "# stopwords.add('and')\n",
    "# stopwords.add('also')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! tokenize training data and remove stop words\n",
    "def preprocess_training_data(training_data, stopwords):\n",
    "    # training_data = [handle_negations(order) for order in training_data]\n",
    "    training_data=[replace_numbers_with_words(order) for order in training_data]\n",
    "    training_data = [word_tokenize(order) for order in training_data]\n",
    "    # print(training_data)\n",
    "    training_data = [[word.lower() for word in order if word.lower() not in stopwords] for order in training_data]\n",
    "    training_data = [clean(order, no_line_breaks=True, no_punct=True, no_currency_symbols=True) for order in training_data]\n",
    "    # print(training_data)\n",
    "    # print(training_data)\n",
    "    #! remove d letter most probably garbage\n",
    "    training_data = [re.sub(r'\\bd\\s+', '', order) for order in training_data]\n",
    "    #! remove \"can\" at the beginning of the sentence\n",
    "    # training_data=[re.sub(r'^can\\s+', '', order) for order in training_data]\n",
    "    training_data = [word_tokenize(order) for order in training_data]\n",
    "    # training_data = [[lemmatizer.lemmatize(word) for word in order] for order in training_data]\n",
    "    training_data = [[lem_word(word) for word in order] for order in training_data]\n",
    "    return training_data\n",
    "training_data = preprocess_training_data(training_data, stopwords)\n",
    "# print(training_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! save processed training dataset\n",
    "path = (\n",
    "    'trian_data_order_category.txt' if datatype == 'train' else\n",
    "    'dev_data_order_category.txt'\n",
    ")\n",
    "with open(path, 'a') as f: #### dev_data_processed.txt\n",
    "    for item in training_data:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! a utility function for extra parentheses ) removal \n",
    "#! handles COMPLEX_TOPPING, NOT,... parenthesis cases\n",
    "def remove_unmatched_parentheses(input_string):\n",
    "    result = list(input_string)  # Convert to list for mutability\n",
    "    last_bracket_index=-1\n",
    "    for i, char in enumerate(result):\n",
    "        if char == ')' and i+2 < len(result):\n",
    "            result[i] = ''  \n",
    "            last_bracket_index=i\n",
    "        elif char == '(':\n",
    "            if last_bracket_index!=-1:\n",
    "                result[last_bracket_index] = ')'\n",
    "                last_bracket_index=-1\n",
    "        elif char == ')' and i+2 >= len(result):\n",
    "            result[i] = ''\n",
    "            result[last_bracket_index] = ')'\n",
    "    return ''.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PIZZAORDER', 'two', 'medium', 'pizzas', 'with', 'sausage', 'and', 'black', 'olives']\n",
      "pizzaorder two medium pizzas with sausage and black olives\n",
      "['pizzaorder', 'two', 'medium', 'pizzas', 'with', 'sausage', 'and', 'black', 'olives']\n",
      "['pizzaorder', 'two', 'medium', 'pizza', 'with', 'sausage', 'and', 'black', 'olive']\n",
      "['PIZZAORDER', 'two', 'medium', 'pizzas', 'with', 'pepperoni', 'and', 'extra', 'cheese']\n",
      "pizzaorder two medium pizzas with pepperoni and extra cheese\n",
      "['pizzaorder', 'two', 'medium', 'pizzas', 'with', 'pepperoni', 'and', 'extra', 'cheese']\n",
      "['pizzaorder', 'two', 'medium', 'pizza', 'with', 'pepperoni', 'and', 'extra', 'cheese']\n",
      "['PIZZAORDER', 'three', 'large', 'pizzas', 'with', 'pepperoni', 'and', 'sausage']\n",
      "pizzaorder three large pizzas with pepperoni and sausage\n",
      "['pizzaorder', 'three', 'large', 'pizzas', 'with', 'pepperoni', 'and', 'sausage']\n",
      "['pizzaorder', 'three', 'large', 'pizza', 'with', 'pepperoni', 'and', 'sausage']\n",
      "['PIZZAORDER', 'five', 'medium', 'pizzas', 'with', 'tomatoes', 'and', 'ham']\n",
      "pizzaorder five medium pizzas with tomatoes and ham\n",
      "['pizzaorder', 'five', 'medium', 'pizzas', 'with', 'tomatoes', 'and', 'ham']\n",
      "['pizzaorder', 'five', 'medium', 'pizza', 'with', 'tomato', 'and', 'ham']\n",
      "['PIZZAORDER', 'one', 'large', 'vegetarian', 'pizza', 'with', 'extra', 'banana', 'peppers']\n",
      "pizzaorder one large vegetarian pizza with extra banana peppers\n",
      "['pizzaorder', 'one', 'large', 'vegetarian', 'pizza', 'with', 'extra', 'banana', 'peppers']\n",
      "['pizzaorder', 'one', 'large', 'vegetarian', 'pizza', 'with', 'extra', 'banana', 'pepper']\n",
      "['PIZZAORDER', 'one', 'large', 'onion', 'and', 'pepper', 'pizza']\n",
      "pizzaorder one large onion and pepper pizza\n",
      "['pizzaorder', 'one', 'large', 'onion', 'and', 'pepper', 'pizza']\n",
      "['pizzaorder', 'one', 'large', 'onion', 'and', 'pepper', 'pizza']\n",
      "['PIZZAORDER', 'one', 'pie', 'along', 'with', 'pesto', 'and', 'ham', 'but', 'avoid', 'olives']\n",
      "pizzaorder one pie along with pesto and ham but avoid olives\n",
      "['pizzaorder', 'one', 'pie', 'along', 'with', 'pesto', 'and', 'ham', 'but', 'avoid', 'olives']\n",
      "['pizzaorder', 'one', 'pie', 'along', 'with', 'pesto', 'and', 'ham', 'but', 'avoid', 'olive']\n",
      "['PIZZAORDER', 'one', 'large', 'pizza', 'with', 'ham', 'bacon', 'onions', 'and', 'black', 'olives']\n",
      "pizzaorder one large pizza with ham bacon onions and black olives\n",
      "['pizzaorder', 'one', 'large', 'pizza', 'with', 'ham', 'bacon', 'onions', 'and', 'black', 'olives']\n",
      "['pizzaorder', 'one', 'large', 'pizza', 'with', 'ham', 'bacon', 'onion', 'and', 'black', 'olive']\n",
      "['PIZZAORDER', 'one', 'medium', 'pizza', 'with', 'sausage', 'and', 'onions']\n",
      "pizzaorder one medium pizza with sausage and onions\n",
      "['pizzaorder', 'one', 'medium', 'pizza', 'with', 'sausage', 'and', 'onions']\n",
      "['pizzaorder', 'one', 'medium', 'pizza', 'with', 'sausage', 'and', 'onion']\n",
      "['DRINKORDER', 'six', 'large', 'cokes']\n",
      "drinkorder six large cokes\n",
      "['drinkorder', 'six', 'large', 'cokes']\n",
      "['drinkorder', 'six', 'large', 'coke']\n",
      "['PIZZAORDER', 'one', 'thin', 'crust', 'medium', 'pizza', 'with', 'tuna', 'but', 'no', 'pineapple']\n",
      "pizzaorder one thin crust medium pizza with tuna but no pineapple\n",
      "['pizzaorder', 'one', 'thin', 'crust', 'medium', 'pizza', 'with', 'tuna', 'but', 'no', 'pineapple']\n",
      "['pizzaorder', 'one', 'thin', 'crust', 'medium', 'pizza', 'with', 'tuna', 'but', 'no', 'pineapple']\n",
      "['PIZZAORDER', 'one', 'pizza', 'with', 'pesto', 'and', 'peppers', 'hold', 'the', 'bacon']\n",
      "pizzaorder one pizza with pesto and peppers hold the bacon\n",
      "['pizzaorder', 'one', 'pizza', 'with', 'pesto', 'and', 'peppers', 'hold', 'the', 'bacon']\n",
      "['pizzaorder', 'one', 'pizza', 'with', 'pesto', 'and', 'pepper', 'hold', 'the', 'bacon']\n",
      "['PIZZAORDER', 'five', 'pies', 'with', 'peppers', 'pesto', 'and', 'onions']\n",
      "pizzaorder five pies with peppers pesto and onions\n",
      "['pizzaorder', 'five', 'pies', 'with', 'peppers', 'pesto', 'and', 'onions']\n",
      "['pizzaorder', 'five', 'pie', 'with', 'pepper', 'pesto', 'and', 'onion']\n",
      "['PIZZAORDER', 'two', 'pies', 'with', 'peppers', 'and', 'bacon', 'pesto']\n",
      "pizzaorder two pies with peppers and bacon pesto\n",
      "['pizzaorder', 'two', 'pies', 'with', 'peppers', 'and', 'bacon', 'pesto']\n",
      "['pizzaorder', 'two', 'pie', 'with', 'pepper', 'and', 'bacon', 'pesto']\n",
      "['PIZZAORDER', 'one', 'large', 'onion', 'and', 'pepperoni', 'pizza', 'with', 'extra', 'cheese']\n",
      "pizzaorder one large onion and pepperoni pizza with extra cheese\n",
      "['pizzaorder', 'one', 'large', 'onion', 'and', 'pepperoni', 'pizza', 'with', 'extra', 'cheese']\n",
      "['pizzaorder', 'one', 'large', 'onion', 'and', 'pepperoni', 'pizza', 'with', 'extra', 'cheese']\n",
      "['PIZZAORDER', 'one', 'medium', 'onion', 'and', 'tuna', 'pie', 'with', 'ham']\n",
      "pizzaorder one medium onion and tuna pie with ham\n",
      "['pizzaorder', 'one', 'medium', 'onion', 'and', 'tuna', 'pie', 'with', 'ham']\n",
      "['pizzaorder', 'one', 'medium', 'onion', 'and', 'tuna', 'pie', 'with', 'ham']\n",
      "['PIZZAORDER', 'one', 'medium', 'mushrooms', 'and', 'olives', 'pie']\n",
      "pizzaorder one medium mushrooms and olives pie\n",
      "['pizzaorder', 'one', 'medium', 'mushrooms', 'and', 'olives', 'pie']\n",
      "['pizzaorder', 'one', 'medium', 'mushroom', 'and', 'olive', 'pie']\n",
      "['PIZZAORDER', 'one', 'small', 'pizza', 'with', 'bacon', 'and', 'olives', 'no', 'pepperoni']\n",
      "pizzaorder one small pizza with bacon and olives no pepperoni\n",
      "['pizzaorder', 'one', 'small', 'pizza', 'with', 'bacon', 'and', 'olives', 'no', 'pepperoni']\n",
      "['pizzaorder', 'one', 'small', 'pizza', 'with', 'bacon', 'and', 'olive', 'no', 'pepperoni']\n",
      "['PIZZAORDER', 'one', 'small', 'pepper', 'and', 'mushroom', 'pizza', 'with', 'no', 'pineapple']\n",
      "pizzaorder one small pepper and mushroom pizza with no pineapple\n",
      "['pizzaorder', 'one', 'small', 'pepper', 'and', 'mushroom', 'pizza', 'with', 'no', 'pineapple']\n",
      "['pizzaorder', 'one', 'small', 'pepper', 'and', 'mushroom', 'pizza', 'with', 'no', 'pineapple']\n",
      "['PIZZAORDER', 'six', 'medium', 'pizzas', 'with', 'tomatoes', 'and', 'pickles']\n",
      "pizzaorder six medium pizzas with tomatoes and pickles\n",
      "['pizzaorder', 'six', 'medium', 'pizzas', 'with', 'tomatoes', 'and', 'pickles']\n",
      "['pizzaorder', 'six', 'medium', 'pizza', 'with', 'tomato', 'and', 'pickle']\n",
      "['PIZZAORDER', 'one', 'medium', 'ham', 'and', 'pineapple', 'pizza']\n",
      "pizzaorder one medium ham and pineapple pizza\n",
      "['pizzaorder', 'one', 'medium', 'ham', 'and', 'pineapple', 'pizza']\n",
      "['pizzaorder', 'one', 'medium', 'ham', 'and', 'pineapple', 'pizza']\n",
      "['DRINKORDER', 'one', 'small', 'iced', 'tea']\n",
      "drinkorder one small iced tea\n",
      "['drinkorder', 'one', 'small', 'iced', 'tea']\n",
      "['drinkorder', 'one', 'small', 'ice', 'tea']\n",
      "['PIZZAORDER', 'two', 'medium', 'pies', 'with', 'peppers', 'and', 'mushrooms', 'but', 'no', 'onions']\n",
      "pizzaorder two medium pies with peppers and mushrooms but no onions\n",
      "['pizzaorder', 'two', 'medium', 'pies', 'with', 'peppers', 'and', 'mushrooms', 'but', 'no', 'onions']\n",
      "['pizzaorder', 'two', 'medium', 'pie', 'with', 'pepper', 'and', 'mushroom', 'but', 'no', 'onion']\n",
      "['PIZZAORDER', 'one', 'medium', 'onion', 'and', 'olive', 'pie', 'with', 'no', 'mushrooms']\n",
      "pizzaorder one medium onion and olive pie with no mushrooms\n",
      "['pizzaorder', 'one', 'medium', 'onion', 'and', 'olive', 'pie', 'with', 'no', 'mushrooms']\n",
      "['pizzaorder', 'one', 'medium', 'onion', 'and', 'olive', 'pie', 'with', 'no', 'mushroom']\n",
      "['PIZZAORDER', 'two', 'large', 'pizzas', 'along', 'with', 'onions', 'thin', 'crust', 'but', 'hold', 'olives']\n",
      "pizzaorder two large pizzas along with onions thin crust but hold olives\n",
      "['pizzaorder', 'two', 'large', 'pizzas', 'along', 'with', 'onions', 'thin', 'crust', 'but', 'hold', 'olives']\n",
      "['pizzaorder', 'two', 'large', 'pizza', 'along', 'with', 'onion', 'thin', 'crust', 'but', 'hold', 'olive']\n",
      "['PIZZAORDER', 'one', 'pizza', 'with', 'onions', 'and', 'ham', 'and', 'tuna']\n",
      "pizzaorder one pizza with onions and ham and tuna\n",
      "['pizzaorder', 'one', 'pizza', 'with', 'onions', 'and', 'ham', 'and', 'tuna']\n",
      "['pizzaorder', 'one', 'pizza', 'with', 'onion', 'and', 'ham', 'and', 'tuna']\n",
      "['PIZZAORDER', 'one', 'medium', 'pizza', 'along', 'with', 'pineapple', 'thin', 'crust', 'but', 'avoid', 'ham']\n",
      "pizzaorder one medium pizza along with pineapple thin crust but avoid ham\n",
      "['pizzaorder', 'one', 'medium', 'pizza', 'along', 'with', 'pineapple', 'thin', 'crust', 'but', 'avoid', 'ham']\n",
      "['pizzaorder', 'one', 'medium', 'pizza', 'along', 'with', 'pineapple', 'thin', 'crust', 'but', 'avoid', 'ham']\n",
      "['PIZZAORDER', 'one', 'large', 'pizza', 'with', 'ham', 'ground', 'beef', 'pepperoni', 'sausage', 'and', 'bacon', 'with', 'extra', 'cheese', 'and', 'extra', 'sauce']\n",
      "pizzaorder one large pizza with ham ground beef pepperoni sausage and bacon with extra cheese and extra sauce\n",
      "['pizzaorder', 'one', 'large', 'pizza', 'with', 'ham', 'ground', 'beef', 'pepperoni', 'sausage', 'and', 'bacon', 'with', 'extra', 'cheese', 'and', 'extra', 'sauce']\n",
      "['pizzaorder', 'one', 'large', 'pizza', 'with', 'ham', 'grind', 'beef', 'pepperoni', 'sausage', 'and', 'bacon', 'with', 'extra', 'cheese', 'and', 'extra', 'sauce']\n",
      "['PIZZAORDER', 'one', 'thin', 'crust', 'sausage', 'pizza', 'with', 'no', 'pesto']\n",
      "pizzaorder one thin crust sausage pizza with no pesto\n",
      "['pizzaorder', 'one', 'thin', 'crust', 'sausage', 'pizza', 'with', 'no', 'pesto']\n",
      "['pizzaorder', 'one', 'thin', 'crust', 'sausage', 'pizza', 'with', 'no', 'pesto']\n",
      "['PIZZAORDER', 'one', 'large', 'big', 'meat', 'pizza']\n",
      "pizzaorder one large big meat pizza\n",
      "['pizzaorder', 'one', 'large', 'big', 'meat', 'pizza']\n",
      "['pizzaorder', 'one', 'large', 'big', 'meat', 'pizza']\n",
      "['DRINKORDER', 'four', 'medium', 'cokes']\n",
      "drinkorder four medium cokes\n",
      "['drinkorder', 'four', 'medium', 'cokes']\n",
      "['drinkorder', 'four', 'medium', 'coke']\n",
      "['PIZZAORDER', 'one', 'large', 'mushroom', 'bacon', 'and', 'peppers', 'pizza']\n",
      "pizzaorder one large mushroom bacon and peppers pizza\n",
      "['pizzaorder', 'one', 'large', 'mushroom', 'bacon', 'and', 'peppers', 'pizza']\n",
      "['pizzaorder', 'one', 'large', 'mushroom', 'bacon', 'and', 'pepper', 'pizza']\n",
      "['PIZZAORDER', 'five', 'medium', 'pizzas', 'with', 'parsley', 'and', 'anchovies']\n",
      "pizzaorder five medium pizzas with parsley and anchovies\n",
      "['pizzaorder', 'five', 'medium', 'pizzas', 'with', 'parsley', 'and', 'anchovies']\n",
      "['pizzaorder', 'five', 'medium', 'pizza', 'with', 'parsley', 'and', 'anchovy']\n",
      "['PIZZAORDER', 'four', 'large', 'pizzas']\n",
      "pizzaorder four large pizzas\n",
      "['pizzaorder', 'four', 'large', 'pizzas']\n",
      "['pizzaorder', 'four', 'large', 'pizza']\n",
      "['PIZZAORDER', 'one', 'cheese', 'pizza']\n",
      "pizzaorder one cheese pizza\n",
      "['pizzaorder', 'one', 'cheese', 'pizza']\n",
      "['pizzaorder', 'one', 'cheese', 'pizza']\n",
      "['DRINKORDER', 'two', 'dr', 'peppers']\n",
      "drinkorder two dr peppers\n",
      "['drinkorder', 'two', 'dr', 'peppers']\n",
      "['drinkorder', 'two', 'dr', 'pepper']\n",
      "['DRINKORDER', 'three', 'pepsis']\n",
      "drinkorder three pepsis\n",
      "['drinkorder', 'three', 'pepsis']\n",
      "['drinkorder', 'three', 'pepsi']\n",
      "['DRINKORDER', 'one', 'sprite']\n",
      "drinkorder one sprite\n",
      "['drinkorder', 'one', 'sprite']\n",
      "['drinkorder', 'one', 'sprite']\n",
      "['PIZZAORDER', 'one', 'small', 'chicken', 'pie', 'with', 'extra', 'cheese', 'hold', 'the', 'ham']\n",
      "pizzaorder one small chicken pie with extra cheese hold the ham\n",
      "['pizzaorder', 'one', 'small', 'chicken', 'pie', 'with', 'extra', 'cheese', 'hold', 'the', 'ham']\n",
      "['pizzaorder', 'one', 'small', 'chicken', 'pie', 'with', 'extra', 'cheese', 'hold', 'the', 'ham']\n"
     ]
    }
   ],
   "source": [
    "#! get PIZZAORDER, DRINKORDER, NONE Labels 0=>PIZZAORDER, 1=>DRINKORDER, 2=>NONE\n",
    "\n",
    "def get_order_category_labels(training_top, training_data, stopwords):\n",
    "    # print(training_data)\n",
    "    order_category_labels = []\n",
    "    for i, item in enumerate(training_top):\n",
    "        order_category_labels.append([2] * len(training_data[i]))\n",
    "        unwanted_keywords = r\"\\b(ORDER|SIZE|STYLE|TOPPING|COMPLEX_TOPPING|QUANTITY|NOT|NUMBER|DRINKTYPE|CONTAINERTYPE|VOLUME)\\b\"\n",
    "        cleaned_string = re.sub('\\('+unwanted_keywords, \"\", item)\n",
    "        cleaned_string = [word for word in cleaned_string.split() if word.lower() not in stopwords]\n",
    "        cleaned_string = ' '.join(cleaned_string)\n",
    "        cleaned_string = remove_unmatched_parentheses(cleaned_string)\n",
    "        order_regex = r\"\\((?:PIZZAORDER|DRINKORDER).*?\\)\"\n",
    "        extracted_orders = re.findall(order_regex, cleaned_string)\n",
    "        k = 0\n",
    "        for order in extracted_orders:\n",
    "            order=replace_numbers_with_words(order)\n",
    "            order = re.sub(r\"[\\(\\)]\", \"\", order)\n",
    "            order=word_tokenize(order) #! fix id and don't bugs\n",
    "            print(order)\n",
    "            order=clean(order, no_line_breaks=True, no_punct=True, no_currency_symbols=True)\n",
    "            order=re.sub(r'\\bd\\s+', '', order) #! for d removal\n",
    "            print(order)\n",
    "            tokens = word_tokenize(order)\n",
    "            print(tokens)\n",
    "            # tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "            tokens = [lem_word(word) for word in tokens]\n",
    "            print(tokens)\n",
    "            j = 0\n",
    "            to_index_train=training_data[i][k:]\n",
    "            if 'pizzaorder' in tokens:\n",
    "                tokens.remove('pizzaorder')\n",
    "                for word in to_index_train:\n",
    "                    if j == len(tokens):\n",
    "                        break\n",
    "                    if word == tokens[j]:\n",
    "                        order_category_labels[i][k] = 0\n",
    "                        j += 1\n",
    "                    k += 1\n",
    "            elif 'drinkorder' in tokens:\n",
    "                tokens.remove('drinkorder')\n",
    "                for word in to_index_train:\n",
    "                    if j == len(tokens):\n",
    "                        break\n",
    "                    if word == tokens[j]:\n",
    "                        order_category_labels[i][k] = 1\n",
    "                        j += 1\n",
    "                    k += 1\n",
    "    return order_category_labels\n",
    "\n",
    "order_category_labels = get_order_category_labels(training_top, training_data, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = (\n",
    "    'train_labels_order_category.txt' if datatype == 'train' else\n",
    "    'dev_labels_order_category.txt'\n",
    ")\n",
    "for labels in order_category_labels:\n",
    "    with open(path, 'a') as f: # dev\n",
    "        f.write(\"%s\\n\" % labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'want', 'to', 'have', 'five', 'pie', 'with', 'pepper', 'pesto', 'and', 'onion', 'please'], ['can', 'i', 'get', 'two', 'pie', 'with', 'pepper', 'and', 'bacon', 'pesto']]\n",
      "[[2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2], [2, 2, 2, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[['i', 'want', 'to', 'have', 'five', 'pie', 'with', 'pepper', 'pesto', 'and', 'onion', 'please'], ['can', 'i', 'get', 'two', 'pie', 'with', 'pepper', 'and', 'bacon', 'pesto']]\n",
      "[[2, 2, 2, 2, 3, 0, 0, 6, 6, 0, 6, 2], [2, 2, 2, 3, 0, 0, 6, 0, 6, 6]]\n"
     ]
    }
   ],
   "source": [
    "def remove_unmatched_parentheses(text):\n",
    "    stack = []\n",
    "    result = list(text)\n",
    "    for i, char in enumerate(text):\n",
    "        if char == '(':\n",
    "            stack.append(i)\n",
    "        elif char == ')':\n",
    "            if stack:\n",
    "                stack.pop()\n",
    "            else:\n",
    "                result[i] = ''  # Remove unmatched closing parenthesis\n",
    "    for i in stack:  # Remove unmatched opening parentheses\n",
    "        result[i] = ''\n",
    "    return ''.join(result)\n",
    "\n",
    "def process_training_top(training_top, stopwords):\n",
    "    labeled_data = []\n",
    "    for i, item in enumerate(training_top):\n",
    "        sentence_tokens = []\n",
    "        labels = []\n",
    "        # Remove unwanted keywords like PIZZAORDER or DRINKORDER\n",
    "        unwanted_keywords = r'\\(\\b(ORDER|PIZZAORDER|DRINKORDER)\\b'\n",
    "        cleaned_string = re.sub(unwanted_keywords, \"\", item)\n",
    "        # Remove stopwords and unnecessary parentheses\n",
    "        words = cleaned_string.split()\n",
    "        cleaned_words = [word for word in words if word.lower() not in stopwords]\n",
    "        cleaned_string = ' '.join(cleaned_words)\n",
    "\n",
    "        # Remove unmatched parentheses\n",
    "        cleaned_string = remove_unmatched_parentheses(cleaned_string)\n",
    "        # Regex to match nested parentheses excluding COMPLEX_TOPPING\n",
    "        pattern_regex = r\"\\((?!COMPLEX_TOPPING\\b)(\\w+)\\s+((?:[^\\(\\)]|\\([^()]*\\))*)\\)\"\n",
    "        matches = list(re.finditer(pattern_regex, cleaned_string))\n",
    "        for _, match in enumerate(matches):\n",
    "            key, value = match.groups()\n",
    "            # Handle NOT block\n",
    "            if key == \"NOT\":\n",
    "                nested_match = re.match(pattern_regex, value.strip())\n",
    "                if nested_match:\n",
    "                    nested_key, nested_value = nested_match.groups()\n",
    "                    value_tokens = word_tokenize(nested_value.strip())\n",
    "                    sentence_tokens.extend(value_tokens)\n",
    "                    labels.extend([f\"NOT_{nested_key}\"] * len(value_tokens))\n",
    "                continue\n",
    "\n",
    "            value = re.sub(r'\\(\\s*\\w+\\s*', '', value)  # Match '(WORD)' for complex toppings\n",
    "            value = re.sub(r'[()]', '', value)  # Remove any remaining parentheses\n",
    "\n",
    "            value_tokens = replace_numbers_with_words(value.strip())\n",
    "            value_tokens = clean(value_tokens, no_line_breaks=True, no_punct=True, no_currency_symbols=True)\n",
    "            value_tokens = re.sub(r'\\bd\\s+', '', value_tokens)  # Remove 'd'\n",
    "            value_tokens = word_tokenize(value_tokens)\n",
    "            value_tokens = [lem_word(word) for word in value_tokens]\n",
    "            sentence_tokens.extend(value_tokens)\n",
    "\n",
    "            # Labeling logic\n",
    "            label_mapping = {\n",
    "                \"STYLE\": \"STYLE\",\n",
    "                \"SIZE\": \"SIZE\",\n",
    "                \"TOPPING\": \"Topping\",\n",
    "                \"NUMBER\": \"Number\",\n",
    "                \"QUANTITY\": \"Complex-Topping\",\n",
    "                \"DRINKTYPE\": \"Drinktype\",\n",
    "                \"CONTAINERTYPE\": \"ContainerType\",\n",
    "            }\n",
    "            labels.extend([label_mapping.get(key, \"None\")] * len(value_tokens))\n",
    "        if sentence_tokens:\n",
    "            labeled_data.append((sentence_tokens, labels))\n",
    "\n",
    "    return labeled_data\n",
    "\n",
    "def update_training_labels(training_data, training_labels, labeled_data):\n",
    "    # Define a mapping for new labels to start from 3\n",
    "    label_mapping = {\n",
    "        \"Number\": 3,\n",
    "        \"SIZE\": 4,\n",
    "        \"STYLE\": 5,\n",
    "        \"Topping\": 6,\n",
    "        \"Complex-Topping\": 7,\n",
    "        \"NOT_TOPPING\": 8,\n",
    "        \"Drinktype\": 9,\n",
    "        \"ContainerType\": 10,\n",
    "        \"None\": 11\n",
    "    }\n",
    "\n",
    "    new_training_data = [list(tokens) for tokens in training_data]\n",
    "    new_training_labels = [list(labels) for labels in training_labels]\n",
    "\n",
    "    for idx, (train_tokens, train_labels) in enumerate(zip(new_training_data, new_training_labels)):\n",
    "        for labeled_tokens, labeled_labels in labeled_data:\n",
    "            if set(labeled_tokens).issubset(set(train_tokens)):\n",
    "                token_to_label = {\n",
    "                    token: label_mapping.get(label, label_mapping[\"None\"]) for token, label in zip(labeled_tokens, labeled_labels)\n",
    "                }\n",
    "                for i, token in enumerate(train_tokens):\n",
    "                    if token in token_to_label:\n",
    "                        train_labels[i] = token_to_label[token]\n",
    "\n",
    "    return new_training_data, new_training_labels\n",
    "\n",
    "labeled_data = process_training_top(training_top, stopwords)\n",
    "Model2_data, Model2_labels = update_training_labels(training_data, order_category_labels, labeled_data)\n",
    "print(training_data[8:10])\n",
    "print(order_category_labels[8:10]) \n",
    "print(Model2_data[8:10])\n",
    "print(Model2_labels[8:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = (\n",
    "    'train_data_order_details.txt' if datatype == 'train' else\n",
    "    'dev_data_order_details.txt'\n",
    ")\n",
    "\n",
    "with open(path, 'a') as f: \n",
    "    for item in Model2_data:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = (\n",
    "    'train_labels_order_details.txt' if datatype == 'train' else\n",
    "    'dev_labels_order_details.txt'\n",
    ")\n",
    "for labels in Model2_labels:\n",
    "    with open(path, 'a') as f:\n",
    "        f.write(\"%s\\n\" % labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
