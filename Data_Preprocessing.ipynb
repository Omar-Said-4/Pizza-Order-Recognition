{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clean-text in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clean-text) (1.7.0)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clean-text) (6.3.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: inflect in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (7.4.0)\n",
      "Requirement already satisfied: more-itertools>=8.5.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from inflect) (10.5.0)\n",
      "Requirement already satisfied: typeguard>=4.0.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from inflect) (4.4.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typeguard>=4.0.1->inflect) (4.12.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install clean-text\n",
    "!pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import inflect\n",
    "from cleantext import clean\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: PIZZA_test.json\n",
      "Lines: 4000\n"
     ]
    }
   ],
   "source": [
    "datatype = 'test' \n",
    "\n",
    "path = (\n",
    "    'PIZZA_train.json' if datatype == 'train' else\n",
    "    'PIZZA_dev.json' if datatype == 'dev' else\n",
    "    'PIZZA_test.json' if datatype == 'test' else\n",
    "    'unknown.json'  # Optional fallback\n",
    ")\n",
    "\n",
    "lines = (\n",
    "    50000 if datatype == 'train' else\n",
    "    847 if datatype == 'dev' else\n",
    "    4000 if datatype == 'test' else\n",
    "    0\n",
    ")\n",
    "\n",
    "print(f\"Path: {path}\")\n",
    "print(f\"Lines: {lines}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#! load data given file path and number of line to load\n",
    "def load_data(file_path,num_lines):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file: \n",
    "        for _ in range(num_lines):\n",
    "            data.append(json.loads(file.readline()))\n",
    "    return data\n",
    "def load_random_data(file_path, num_lines, last_lines):\n",
    "    data = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        file.seek(0, 2)\n",
    "        file_size = file.tell()\n",
    "        \n",
    "        file.seek(max(file_size - 1024 * last_lines, 0))  \n",
    "        lines = file.readlines()\n",
    "        \n",
    "        last_lines_data = []\n",
    "        for line in lines[-last_lines:]:\n",
    "            if line.strip():\n",
    "                try:\n",
    "                    last_lines_data.append(json.loads(line.strip()))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Skipping invalid JSON line: {line.strip()} - {e}\")\n",
    "        \n",
    "        data.extend(last_lines_data)\n",
    "\n",
    "        remaining_lines_size = file_size - sum(len(line) for line in lines[-last_lines:])\n",
    "\n",
    "        for _ in range(num_lines):\n",
    "            while True:\n",
    "                try:\n",
    "                    random_pos = random.randint(0, remaining_lines_size - 1)\n",
    "                    file.seek(random_pos)\n",
    "                    \n",
    "                    file.readline() \n",
    "                    line = file.readline() \n",
    "\n",
    "                    if line.strip(): \n",
    "                        data.append(json.loads(line.strip()))\n",
    "                        break \n",
    "                except json.JSONDecodeError as e:\n",
    "                    continue  \n",
    "                except Exception as e:\n",
    "                    print(f\"Error during random data load: {e}\")\n",
    "                    break\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dev.SRC': 'i want to order two medium pizzas with sausage and black olives and two medium pizzas with pepperoni and extra cheese and three large pizzas with pepperoni and sausage', 'dev.EXR': '(ORDER (PIZZAORDER (NUMBER 2 ) (SIZE MEDIUM ) (COMPLEX_TOPPING (QUANTITY EXTRA ) (TOPPING CHEESE ) ) (TOPPING PEPPERONI ) ) (PIZZAORDER (NUMBER 2 ) (SIZE MEDIUM ) (TOPPING OLIVES ) (TOPPING SAUSAGE ) ) (PIZZAORDER (NUMBER 3 ) (SIZE LARGE ) (TOPPING PEPPERONI ) (TOPPING SAUSAGE ) ) )', 'dev.TOP': '(ORDER i want to order (PIZZAORDER (NUMBER two ) (SIZE medium ) pizzas with (TOPPING sausage ) and (TOPPING black olives ) ) and (PIZZAORDER (NUMBER two ) (SIZE medium ) pizzas with (TOPPING pepperoni ) and (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING cheese ) ) ) and (PIZZAORDER (NUMBER three ) (SIZE large ) pizzas with (TOPPING pepperoni ) and (TOPPING sausage ) ) )', 'dev.PCFG_ERR': 'False'}, {'dev.SRC': 'five medium pizzas with tomatoes and ham', 'dev.EXR': '(ORDER (PIZZAORDER (NUMBER 5 ) (SIZE MEDIUM ) (TOPPING HAM ) (TOPPING TOMATOES ) ) )', 'dev.TOP': '(ORDER (PIZZAORDER (NUMBER five ) (SIZE medium ) pizzas with (TOPPING tomatoes ) and (TOPPING ham ) ) )', 'dev.PCFG_ERR': 'False'}, {'dev.SRC': 'i need to order one large vegetarian pizza with extra banana peppers', 'dev.EXR': '(ORDER (PIZZAORDER (NUMBER 1 ) (SIZE LARGE ) (STYLE VEGETARIAN ) (COMPLEX_TOPPING (QUANTITY EXTRA ) (TOPPING BANANA_PEPPERS ) ) ) )', 'dev.TOP': '(ORDER i need to order (PIZZAORDER (NUMBER one ) (SIZE large ) (STYLE vegetarian ) pizza with (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING banana peppers ) ) ) )', 'dev.PCFG_ERR': 'False'}]\n"
     ]
    }
   ],
   "source": [
    "data=load_random_data(path,lines,1000) if datatype=='train' else load_data(path,lines)\n",
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i want to order two medium pizzas with sausage and black olives and two medium pizzas with pepperoni and extra cheese and three large pizzas with pepperoni and sausage', 'five medium pizzas with tomatoes and ham']\n",
      "['(ORDER (PIZZAORDER (NUMBER 2 ) (SIZE MEDIUM ) (COMPLEX_TOPPING (QUANTITY EXTRA ) (TOPPING CHEESE ) ) (TOPPING PEPPERONI ) ) (PIZZAORDER (NUMBER 2 ) (SIZE MEDIUM ) (TOPPING OLIVES ) (TOPPING SAUSAGE ) ) (PIZZAORDER (NUMBER 3 ) (SIZE LARGE ) (TOPPING PEPPERONI ) (TOPPING SAUSAGE ) ) )', '(ORDER (PIZZAORDER (NUMBER 5 ) (SIZE MEDIUM ) (TOPPING HAM ) (TOPPING TOMATOES ) ) )']\n",
      "['(ORDER i want to order (PIZZAORDER (NUMBER two ) (SIZE medium ) pizzas with (TOPPING sausage ) and (TOPPING black olives ) ) and (PIZZAORDER (NUMBER two ) (SIZE medium ) pizzas with (TOPPING pepperoni ) and (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING cheese ) ) ) and (PIZZAORDER (NUMBER three ) (SIZE large ) pizzas with (TOPPING pepperoni ) and (TOPPING sausage ) ) )', '(ORDER (PIZZAORDER (NUMBER five ) (SIZE medium ) pizzas with (TOPPING tomatoes ) and (TOPPING ham ) ) )']\n"
     ]
    }
   ],
   "source": [
    "#! split json data\n",
    "def getextensions(datatype):\n",
    "    if datatype=='train':\n",
    "        return 'train.SRC','train.EXR','train.TOP'\n",
    "    elif datatype=='dev':\n",
    "        return 'dev.SRC','dev.EXR','dev.TOP'\n",
    "    else:\n",
    "        return 'test.SRC','test.EXR','test.TOP'\n",
    "def get_training_data(data,datatype='train'):\n",
    "    values=getextensions(datatype)\n",
    "    training_data = []\n",
    "    training_exr=[]\n",
    "    training_top=[]\n",
    "    for item in data:\n",
    "        training_data.append(item[values[0]])  \n",
    "        training_exr.append(item[values[1]])  \n",
    "        training_top.append(item[values[2]]) \n",
    "    return training_data,training_exr,training_top\n",
    "training_data,training_exr,training_top=get_training_data(data,datatype)   \n",
    "print(training_data[:2])\n",
    "print(training_exr[:2])\n",
    "print(training_top[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "place an order for a meduim pizzas and three cokes\n"
     ]
    }
   ],
   "source": [
    "p = inflect.engine()\n",
    "\n",
    "def replace_numbers_with_words(text):\n",
    "    number_pattern = r'\\b\\d+\\b'\n",
    "    \n",
    "    def number_to_words(match):\n",
    "        number = int(match.group())\n",
    "        return p.number_to_words(number, andword=\"\").replace('-', '_')\n",
    "\n",
    "    \n",
    "    text = re.sub(number_pattern, number_to_words, text)\n",
    "    \n",
    "    # Replace standalone 'a' or 'an' with 'one' where appropriate\n",
    "    text = re.sub(\n",
    "        r'\\b(a|an)\\b(?! ((lot)|(little)|(few)|(number)|(couple)|(bit)|(load)|(stack)|(bunch)|(group)|(set)|(series)|(variety)|(range)|(amount)|(sum)|(total)))',\n",
    "       'one',\n",
    "        text,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "print(replace_numbers_with_words(\"place an order for a meduim pizzas and 3 cokes\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! a function to handle negations\n",
    "def handle_negations(text):\n",
    "    negations_pattern = r\"\\b(?:no|not|without)\\s+.*?\\b(?=(?:[^\\w\\s]|$))\"\n",
    "    # print(re.findall(negations_pattern, text))\n",
    "    text = re.sub(negations_pattern, lambda x: ' '.join([f'not_{word}' for word in x.group(0).split()]), text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_word(word):\n",
    "    if(word=='bit'):\n",
    "        return word\n",
    "    possible_pos = [wordnet.NOUN, wordnet.VERB, wordnet.ADJ, wordnet.ADV]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for pos in possible_pos:\n",
    "        word=lemmatizer.lemmatize(word,pos)\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "#! stopwords list (adding not_)\n",
    "stopwords = set(stopwords.words('english'))\n",
    "stopwords.add('like')\n",
    "not_stopwords = ['not_' + word for word in stopwords]\n",
    "# stopwords.update(not_stopwords)\n",
    "# stopwords.discard('a')\n",
    "# stopwords.discard('an')\n",
    "# stopwords.discard('not')\n",
    "# stopwords.discard('no')\n",
    "# stopwords.discard('can')\n",
    "# stopwords.discard('not_a')\n",
    "# stopwords.discard('not_an')\n",
    "# stopwords.discard('not_can')\n",
    "# stopwords.discard('not_no')\n",
    "stopwords=set()\n",
    "# stopwords.add('and')\n",
    "# stopwords.add('also')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Read the CSV file\n",
    "# data = pd.read_csv('test_set.csv')\n",
    "\n",
    "# # Access the 'order' column\n",
    "# orders = data['order']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! tokenize training data and remove stop words\n",
    "def preprocess_training_data(training_data, stopwords):\n",
    "    training_data=[replace_numbers_with_words(order) for order in training_data]\n",
    "    training_data = [word_tokenize(order) for order in training_data]\n",
    "    training_data = [[word.lower() for word in order if word.lower() not in stopwords] for order in training_data]\n",
    "    training_data = [clean(order, no_line_breaks=True, no_punct=True, no_currency_symbols=True) for order in training_data]\n",
    "    #! remove d letter most probably garbage\n",
    "    training_data = [re.sub(r'\\bd\\s+', '', order) for order in training_data]\n",
    "    training_data = [word_tokenize(order) for order in training_data]\n",
    "    training_data = [[lem_word(word) for word in order] for order in training_data]\n",
    "    return training_data\n",
    "training_data = preprocess_training_data(training_data, stopwords)\n",
    "print(training_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! save processed training dataset\n",
    "path = (\n",
    "    'trian_data_order_category.txt' if datatype == 'train' else\n",
    "    'dev_data_order_category.txt' if datatype == 'dev' else\n",
    "    'test_data_order_category2.txt'\n",
    ")\n",
    "with open(path, 'a') as f:\n",
    "    for item in training_data:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_unmatched_parentheses(input_string):\n",
    "    result = list(input_string) \n",
    "    last_bracket_index=-1\n",
    "    for i, char in enumerate(result):\n",
    "        if char == ')' and i+2 < len(result):\n",
    "            result[i] = ''  \n",
    "            last_bracket_index=i\n",
    "        elif char == '(':\n",
    "            if last_bracket_index!=-1:\n",
    "                result[last_bracket_index] = ')'\n",
    "                last_bracket_index=-1\n",
    "        elif char == ')' and i+2 >= len(result):\n",
    "            result[i] = ''\n",
    "            result[last_bracket_index] = ')'\n",
    "    return ''.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! get PIZZAORDER, DRINKORDER, NONE Labels 0=>PIZZAORDER, 1=>DRINKORDER, 2=>NONE\n",
    "\n",
    "def get_order_category_labels(training_top, training_data, stopwords):\n",
    "    # print(training_data)\n",
    "    order_category_labels = []\n",
    "    for i, item in enumerate(training_top):\n",
    "        order_category_labels.append([2] * len(training_data[i]))\n",
    "        #! remove any extra labelling\n",
    "        item = re.sub(r'\\)[^()]*\\(', ') (', item)\n",
    "        unwanted_keywords = r\"\\b(ORDER|SIZE|STYLE|TOPPING|COMPLEX_TOPPING|QUANTITY|NOT|NUMBER|DRINKTYPE|CONTAINERTYPE|VOLUME)\\b\"\n",
    "        cleaned_string = re.sub('\\('+unwanted_keywords, \"\", item)\n",
    "        cleaned_string = [word for word in cleaned_string.split() if word.lower() not in stopwords]\n",
    "        cleaned_string = ' '.join(cleaned_string)\n",
    "        cleaned_string = remove_unmatched_parentheses(cleaned_string)\n",
    "        order_regex = r\"\\((?:PIZZAORDER|DRINKORDER).*?\\)\"\n",
    "        extracted_orders = re.findall(order_regex, cleaned_string)\n",
    "        k = 0\n",
    "        for order in extracted_orders:\n",
    "            order=replace_numbers_with_words(order)\n",
    "            order = re.sub(r\"[\\(\\)]\", \"\", order)\n",
    "            order=word_tokenize(order) #! fix id and don't bugs\n",
    "            order=clean(order, no_line_breaks=True, no_punct=True, no_currency_symbols=True)\n",
    "            order=re.sub(r'\\bd\\s+', '', order) #! for d removal\n",
    "            tokens = word_tokenize(order)\n",
    "            tokens = [lem_word(word) for word in tokens]\n",
    "            j = 0\n",
    "            to_index_train=training_data[i][k:]\n",
    "            if 'pizzaorder' in tokens:\n",
    "                tokens.remove('pizzaorder')\n",
    "                for word in to_index_train:\n",
    "                    if j == len(tokens):\n",
    "                        break\n",
    "                    if word == tokens[j]:\n",
    "                        order_category_labels[i][k] = 0\n",
    "                        j += 1\n",
    "                    k += 1\n",
    "                 #! EOP label\n",
    "                # order_category_labels[i][k-1] = 3\n",
    "            elif 'drinkorder' in tokens:\n",
    "                tokens.remove('drinkorder')\n",
    "                for word in to_index_train:\n",
    "                    if j == len(tokens):\n",
    "                        break\n",
    "                    if word == tokens[j]:\n",
    "                        order_category_labels[i][k] = 1\n",
    "                        j += 1\n",
    "                    k += 1\n",
    "                #! EOD label\n",
    "                # order_category_labels[i][k-1] = 4\n",
    "    return order_category_labels\n",
    "\n",
    "order_category_labels = get_order_category_labels(training_top, training_data, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = (\n",
    "    'train_labels_order_category.txt' if datatype == 'train' else\n",
    "    'dev_labels_order_category.txt'\n",
    ")\n",
    "for labels in order_category_labels:\n",
    "    with open(path, 'a') as f: # dev\n",
    "        f.write(\"%s\\n\" % labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['order', 'one', 'extra', 'cheese', 'and', 'pepper', 'pizza', 'without', 'pesto'], ['i', 'll', 'try', 'one', 'small', 'pie', 'along', 'with', 'onion', 'mushroom', 'but', 'hold', 'pesto']]\n",
      "[[2, 0, 0, 0, 2, 0, 2, 2, 0], [2, 2, 2, 0, 0, 2, 2, 2, 0, 0, 2, 2, 0]]\n",
      "[['order', 'one', 'extra', 'cheese', 'and', 'pepper', 'pizza', 'without', 'pesto'], ['i', 'll', 'try', 'one', 'small', 'pie', 'along', 'with', 'onion', 'mushroom', 'but', 'hold', 'pesto']]\n",
      "[[0, 1, 5, 4, 0, 4, 0, 0, 6], [0, 0, 0, 1, 2, 0, 0, 0, 4, 4, 0, 0, 6]]\n"
     ]
    }
   ],
   "source": [
    "def remove_unmatched_parentheses(text):\n",
    "    stack = []\n",
    "    result = list(text)\n",
    "    for i, char in enumerate(text):\n",
    "        if char == '(':\n",
    "            stack.append(i)\n",
    "        elif char == ')':\n",
    "            if stack:\n",
    "                stack.pop()\n",
    "            else:\n",
    "                result[i] = ''  \n",
    "    for i in stack:\n",
    "        result[i] = ''\n",
    "    return ''.join(result)\n",
    "\n",
    "def process_training_top(training_top, stopwords):\n",
    "    labeled_data = []\n",
    "    for i, item in enumerate(training_top):\n",
    "        sentence_tokens = []\n",
    "        labels = []\n",
    "        unwanted_keywords = r'\\(\\b(ORDER|PIZZAORDER|DRINKORDER)\\b'\n",
    "        cleaned_string = re.sub(unwanted_keywords, \"\", item)\n",
    "        words = cleaned_string.split()\n",
    "        cleaned_words = [word for word in words if word.lower() not in stopwords]\n",
    "        cleaned_string = ' '.join(cleaned_words)\n",
    "\n",
    "        cleaned_string = remove_unmatched_parentheses(cleaned_string)\n",
    "        # Regex to match nested parentheses excluding COMPLEX_TOPPING\n",
    "        pattern_regex = r\"\\((?!COMPLEX_TOPPING\\b)(\\w+)\\s+((?:[^\\(\\)]|\\([^()]*\\))*)\\)\"\n",
    "        matches = list(re.finditer(pattern_regex, cleaned_string))\n",
    "        for _, match in enumerate(matches):\n",
    "            key, value = match.groups()\n",
    "            # Handle NOT block\n",
    "            if key == \"NOT\":\n",
    "                nested_match = re.match(pattern_regex, value.strip())\n",
    "                if nested_match:\n",
    "                    nested_key, nested_value = nested_match.groups()\n",
    "                    nested_value=replace_numbers_with_words(nested_value)\n",
    "                    nested_value = clean(nested_value, no_line_breaks=True, no_punct=True, no_currency_symbols=True)\n",
    "                    nested_value = re.sub(r'\\bd\\s+', '', nested_value)  # Remove 'd'\n",
    "                    value_tokens = word_tokenize(nested_value.strip())\n",
    "                    value_tokens = [lem_word(word) for word in value_tokens]\n",
    "                    sentence_tokens.extend(value_tokens)\n",
    "                    labels.extend([f\"NOT_{nested_key}\"] * len(value_tokens))\n",
    "                continue\n",
    "\n",
    "            value = re.sub(r'\\(\\s*\\w+\\s*', '', value)  # Match '(WORD)' for complex toppings\n",
    "            value = re.sub(r'[()]', '', value)  # Remove any remaining parentheses\n",
    "            value_tokens = replace_numbers_with_words(value.strip())\n",
    "            value_tokens = clean(value_tokens, no_line_breaks=True, no_punct=True, no_currency_symbols=True)\n",
    "            value_tokens = re.sub(r'\\bd\\s+', '', value_tokens) \n",
    "            value_tokens = word_tokenize(value_tokens)\n",
    "            value_tokens = [lem_word(word) for word in value_tokens]\n",
    "            sentence_tokens.extend(value_tokens)\n",
    "\n",
    "            label_mapping = {\n",
    "                \"STYLE\": \"STYLE\",\n",
    "                \"SIZE\": \"SIZE\",\n",
    "                \"TOPPING\": \"Topping\",\n",
    "                \"NUMBER\": \"Number\",\n",
    "                \"QUANTITY\": \"Complex-Topping\",\n",
    "                \"DRINKTYPE\": \"Drinktype\",\n",
    "                \"CONTAINERTYPE\": \"ContainerType\",\n",
    "                \"VOLUME\":\"Volume\"\n",
    "            }\n",
    "            labels.extend([label_mapping.get(key, \"None\")] * len(value_tokens))\n",
    "        if sentence_tokens:\n",
    "            labeled_data.append((sentence_tokens, labels))\n",
    "\n",
    "    return labeled_data\n",
    "\n",
    "def update_training_labels(training_data, training_labels, labeled_data,training_top):\n",
    "    label_mapping = {\n",
    "    \"None\": 0,\n",
    "    \"Number\": 1,\n",
    "    \"SIZE\": 2,\n",
    "    \"STYLE\": 3,\n",
    "    \"Topping\": 4,\n",
    "    \"Complex-Topping\": 5,\n",
    "    \"NOT_TOPPING\": 6,\n",
    "    \"NOT_STYLE\": 7,\n",
    "    \"Drinktype\": 8,\n",
    "    \"ContainerType\": 9,\n",
    "    \"Volume\": 10,\n",
    "    }\n",
    "    new_training_data = [list(tokens) for tokens in training_data]\n",
    "    new_training_labels = [[0 for _ in labels] for labels in training_labels]\n",
    "    for idx, (train_tokens, train_labels) in enumerate(zip(new_training_data, new_training_labels)):\n",
    "        labeled_tokens, labeled_labels = labeled_data[idx] \n",
    "        train_ptr = 0\n",
    "        labeled_ptr = 0\n",
    "        \n",
    "        while train_ptr < len(train_tokens) and labeled_ptr < len(labeled_tokens):\n",
    "            if train_tokens[train_ptr] == labeled_tokens[labeled_ptr]:\n",
    "                train_labels[train_ptr] = label_mapping.get(labeled_labels[labeled_ptr], label_mapping[\"None\"])\n",
    "                labeled_ptr += 1  # Move labeled_tokens pointer\n",
    "            train_ptr += 1  # Move training_tokens pointer\n",
    "        \n",
    "        if labeled_ptr != len(labeled_tokens):\n",
    "            print(f\"Warning: Not all labeled tokens found in training tokens for index {idx}.\")\n",
    "            print(labeled_tokens)\n",
    "            print(train_tokens)\n",
    "            print(training_top[idx])\n",
    "    return new_training_data, new_training_labels\n",
    "labeled_data = process_training_top(training_top, stopwords)\n",
    "Model2_data, Model2_labels = update_training_labels(training_data, order_category_labels, labeled_data,training_top)\n",
    "print(training_data[168:170])\n",
    "print(order_category_labels[168:170]) \n",
    "print(Model2_data[168:170])\n",
    "print(Model2_labels[168:170])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = (\n",
    "    'train_data_order_details.txt' if datatype == 'train' else\n",
    "    'dev_data_order_details.txt'\n",
    ")\n",
    "\n",
    "with open(path, 'a') as f: \n",
    "    for item in Model2_data:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = (\n",
    "    'train_labels_order_details.txt' if datatype == 'train' else\n",
    "    'dev_labels_order_details.txt'\n",
    ")\n",
    "for labels in Model2_labels:\n",
    "    with open(path, 'a') as f:\n",
    "        f.write(\"%s\\n\" % labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
