{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clean-text in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clean-text) (1.7.0)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clean-text) (6.3.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: inflect in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (7.4.0)\n",
      "Requirement already satisfied: more-itertools>=8.5.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from inflect) (10.5.0)\n",
      "Requirement already satisfied: typeguard>=4.0.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from inflect) (4.4.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typeguard>=4.0.1->inflect) (4.12.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install clean-text\n",
    "!pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import inflect\n",
    "from cleantext import clean\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: PIZZA_train.json\n",
      "Lines: 2457303\n"
     ]
    }
   ],
   "source": [
    "datatype = 'train' \n",
    "\n",
    "path = (\n",
    "    'PIZZA_train.json' if datatype == 'train' else\n",
    "    'PIZZA_dev.json' if datatype == 'dev' else\n",
    "    'PIZZA_test.json' if datatype == 'test' else\n",
    "    'unknown.json'  # Optional fallback\n",
    ")\n",
    "\n",
    "lines = (\n",
    "    2457303 if datatype == 'train' else\n",
    "    847 if datatype == 'dev' else\n",
    "    1357 if datatype == 'test' else\n",
    "    0  \n",
    ")\n",
    "\n",
    "print(f\"Path: {path}\")\n",
    "print(f\"Lines: {lines}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#! load data given file path and number of line to load\n",
    "def load_data(file_path,num_lines):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file: \n",
    "        for _ in range(num_lines):\n",
    "            data.append(json.loads(file.readline())) \n",
    "    return data\n",
    "def load_random_data(file_path, num_lines, last_lines):\n",
    "    data = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        file.seek(0, 2)  # Move to the end of the file to get its size\n",
    "        file_size = file.tell()\n",
    "        \n",
    "        # Read the last `last_lines` lines\n",
    "        file.seek(max(file_size - 1024 * last_lines, 0))  # Optional: Adjust the seek window size for efficiency\n",
    "        lines = file.readlines()\n",
    "        last_lines_data = [json.loads(line.strip()) for line in lines[-last_lines:] if line.strip()]\n",
    "        data.extend(last_lines_data)\n",
    "\n",
    "        # Read `num_lines` random lines from the rest of the file (excluding the last `last_lines` lines)\n",
    "        remaining_lines = file_size - sum(len(line) for line in lines[-last_lines:])\n",
    "        for _ in range(num_lines):\n",
    "            while True:\n",
    "                # Pick a random position outside the last `last_lines` range\n",
    "                random_pos = random.randint(0, remaining_lines - 1)\n",
    "                file.seek(random_pos)\n",
    "                \n",
    "                # Read to the end of the current line to avoid partial lines\n",
    "                file.readline()\n",
    "                line = file.readline()  # Read the next line (complete line)\n",
    "                if line:  # Ensure it's not an empty line\n",
    "                    # Check if it's not from the last `last_lines` lines\n",
    "                    # if any(line.strip() == last_line for last_line in last_lines_data):\n",
    "                    #     continue  # Skip if it's from the last lines\n",
    "                    try:\n",
    "                        data.append(json.loads(line.strip()))\n",
    "                        break  # Exit the loop for this line\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue  # Skip if it's not valid JSON\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_data(path,lines) if datatype=='train' else load_data(path,lines)\n",
    "print(data[:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i'd like a pizza with jalapeno peppers chicken and black olives without thin crust\", \"i'd like a pizza with sausages anchovies and jalapeno pepper without thin crust\"]\n",
      "['(ORDER (PIZZAORDER (NUMBER 1 ) (TOPPING JALAPENO_PEPPERS ) (TOPPING CHICKEN ) (TOPPING OLIVES ) (NOT (STYLE THIN_CRUST ) ) ) )', '(ORDER (PIZZAORDER (NUMBER 1 ) (TOPPING SAUSAGE ) (TOPPING ANCHOVIES ) (TOPPING JALAPENO_PEPPERS ) (NOT (STYLE THIN_CRUST ) ) ) )']\n",
      "[\"(ORDER i'd like (PIZZAORDER (NUMBER a ) pizza with (TOPPING jalapeno peppers ) (TOPPING chicken ) and (TOPPING black olives ) without (NOT (STYLE thin crust ) ) ) )\", \"(ORDER i'd like (PIZZAORDER (NUMBER a ) pizza with (TOPPING sausages ) (TOPPING anchovies ) and (TOPPING jalapeno pepper ) without (NOT (STYLE thin crust ) ) ) )\"]\n"
     ]
    }
   ],
   "source": [
    "#! split json data\n",
    "def getextensions(datatype):\n",
    "    if datatype=='train':\n",
    "        return 'train.SRC','train.EXR','train.TOP'\n",
    "    elif datatype=='dev':\n",
    "        return 'dev.SRC','dev.EXR','dev.TOP'\n",
    "    else:\n",
    "        return 'test.SRC','test.EXR','test.TOP'\n",
    "def get_training_data(data,datatype='train'):\n",
    "    values=getextensions(datatype)\n",
    "    training_data = []\n",
    "    training_exr=[]\n",
    "    training_top=[]\n",
    "    # training_top_dec=[]\n",
    "    for item in data:\n",
    "        training_data.append(item[values[0]])  \n",
    "        training_exr.append(item[values[1]])  \n",
    "        training_top.append(item[values[2]])\n",
    "        # if datatype=='train':\n",
    "        #     training_top_dec.append(item[values[3]])  \n",
    "    return training_data,training_exr,training_top\n",
    "training_data,training_exr,training_top=get_training_data(data,datatype)   \n",
    "print(training_data[:2])\n",
    "print(training_exr[:2])\n",
    "print(training_top[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an inflect engine\n",
    "p = inflect.engine()\n",
    "\n",
    "def replace_numbers_with_words(text):\n",
    "    # Regular expression to find numbers in the text\n",
    "    number_pattern = r'\\b\\d+\\b'\n",
    "    \n",
    "    # Function to convert a number match to its word representation\n",
    "    def number_to_words(match):\n",
    "        number = int(match.group())\n",
    "        # Convert to words and replace hyphens with underscores\n",
    "        return p.number_to_words(number, andword=\"\").replace('-', '_')\n",
    "    \n",
    "    # Substitute numbers with their word representation\n",
    "    text = re.sub(number_pattern, number_to_words, text)\n",
    "    \n",
    "    # Replace the standalone 'a' with 'one'\n",
    "    text = re.sub(r'\\b[aA]\\b', 'one', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! a function to handle negations\n",
    "def handle_negations(text):\n",
    "    #! negations are based on training set\n",
    "    negations_pattern = r\"\\b(?:no|not|without)\\s+.*?\\b(?=(?:[^\\w\\s]|$))\"\n",
    "    # print(re.findall(negations_pattern, text))\n",
    "    text = re.sub(negations_pattern, lambda x: ' '.join([f'not_{word}' for word in x.group(0).split()]), text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!# lemmatize words with all possible pos tags\n",
    "def lem_word(word):\n",
    "    possible_pos = [wordnet.NOUN, wordnet.VERB, wordnet.ADJ, wordnet.ADV]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for pos in possible_pos:\n",
    "        word=lemmatizer.lemmatize(word,pos)\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "#! stopwords list (adding not_)\n",
    "stopwords = set(stopwords.words('english'))\n",
    "stopwords.add('like')\n",
    "not_stopwords = ['not_' + word for word in stopwords]\n",
    "stopwords.update(not_stopwords)\n",
    "# stopwords.discard('a')\n",
    "# stopwords.discard('an')\n",
    "# stopwords.discard('not')\n",
    "# stopwords.discard('no')\n",
    "# stopwords.discard('can')\n",
    "# stopwords.discard('not_a')\n",
    "# stopwords.discard('not_an')\n",
    "# stopwords.discard('not_can')\n",
    "# stopwords.discard('not_no')\n",
    "stopwords=set()\n",
    "# stopwords.add('and')\n",
    "# stopwords.add('also')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'like', 'one', 'pizza', 'with', 'jalapeno', 'pepper', 'chicken', 'and', 'black', 'olive', 'without', 'thin', 'crust'], ['i', 'like', 'one', 'pizza', 'with', 'sausage', 'anchovy', 'and', 'jalapeno', 'pepper', 'without', 'thin', 'crust'], ['i', 'like', 'one', 'pizza', 'with', 'caramelize', 'red', 'onion', 'pea', 'and', 'bean', 'without', 'thin', 'crust'], ['i', 'like', 'one', 'pizza', 'with', 'peperoni', 'buffalo', 'sauce', 'and', 'pecorino', 'without', 'thin', 'crust'], ['i', 'like', 'one', 'pizza', 'with', 'mozzarella', 'pepperoni', 'and', 'pesto', 'sauce', 'without', 'thin', 'crust']]\n"
     ]
    }
   ],
   "source": [
    "#! tokenize training data and remove stop words\n",
    "def preprocess_training_data(training_data, stopwords):\n",
    "    # training_data = [handle_negations(order) for order in training_data]\n",
    "    training_data=[replace_numbers_with_words(order) for order in training_data]\n",
    "    training_data = [word_tokenize(order) for order in training_data]\n",
    "    training_data = [[word.lower() for word in order if word.lower() not in stopwords] for order in training_data]\n",
    "    training_data = [clean(order, no_line_breaks=True, no_punct=True, no_currency_symbols=True) for order in training_data]\n",
    "    # print(training_data)\n",
    "    #! remove d letter most probably garbage\n",
    "    training_data = [re.sub(r'\\bd\\s+', '', order) for order in training_data]\n",
    "    #! remove \"can\" at the beginning of the sentence\n",
    "    # training_data=[re.sub(r'^can\\s+', '', order) for order in training_data]\n",
    "    training_data = [word_tokenize(order) for order in training_data]\n",
    "    # training_data = [[lemmatizer.lemmatize(word) for word in order] for order in training_data]\n",
    "    training_data = [[lem_word(word) for word in order] for order in training_data]\n",
    "    return training_data\n",
    "training_data = preprocess_training_data(training_data, stopwords)\n",
    "print(training_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! save processed training dataset\n",
    "path = (\n",
    "    'training_data_processed_1.txt' if datatype == 'train' else\n",
    "    'dev_data_processed_1.txt' if datatype == 'dev' else\n",
    "    'dev_data_processed.txt' if datatype == 'test' else\n",
    "    'unknown_data_processed.txt'  # Optional fallback\n",
    ")\n",
    "\n",
    "with open(path, 'a') as f: #### dev_data_processed.txt\n",
    "    for item in training_data:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! a utility function for extra parentheses ) removal \n",
    "#! handles COMPLEX_TOPPING, NOT,... parenthesis cases\n",
    "def remove_unmatched_parentheses(input_string):\n",
    "    result = list(input_string)  # Convert to list for mutability\n",
    "    last_bracket_index=-1\n",
    "    for i, char in enumerate(result):\n",
    "        if char == ')' and i+2 < len(result):\n",
    "            result[i] = ''  \n",
    "            last_bracket_index=i\n",
    "        elif char == '(':\n",
    "            if last_bracket_index!=-1:\n",
    "                result[last_bracket_index] = ')'\n",
    "                last_bracket_index=-1\n",
    "        elif char == ')' and i+2 >= len(result):\n",
    "            result[i] = ''\n",
    "            result[last_bracket_index] = ')'\n",
    "    return ''.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "#! get PIZZAORDER, DRINKORDER, NONE Labels 0=>PIZZAORDER, 1=>DRINKORDER, 2=>NONE\n",
    "\n",
    "def get_order_category_labels(training_top, training_data, stopwords):\n",
    "    # print(training_data)\n",
    "    order_category_labels = []\n",
    "    for i, item in enumerate(training_top):\n",
    "        order_category_labels.append([2] * len(training_data[i]))\n",
    "        unwanted_keywords = r\"\\b(ORDER|SIZE|STYLE|TOPPING|COMPLEX_TOPPING|QUANTITY|NOT|NUMBER|DRINKTYPE|CONTAINERTYPE|VOLUME)\\b\"\n",
    "        cleaned_string = re.sub('\\('+unwanted_keywords, \"\", item)\n",
    "        cleaned_string = [word for word in cleaned_string.split() if word.lower() not in stopwords]\n",
    "        cleaned_string = ' '.join(cleaned_string)\n",
    "        cleaned_string = remove_unmatched_parentheses(cleaned_string)\n",
    "        order_regex = r\"\\((?:PIZZAORDER|DRINKORDER).*?\\)\"\n",
    "        extracted_orders = re.findall(order_regex, cleaned_string)\n",
    "        k = 0\n",
    "        for order in extracted_orders:\n",
    "            order=replace_numbers_with_words(order)\n",
    "            order = re.sub(r\"[\\(\\)]\", \"\", order)\n",
    "            order=word_tokenize(order) #! fix id and don't bugs\n",
    "            order=clean(order, no_line_breaks=True, no_punct=True, no_currency_symbols=True)\n",
    "            order=re.sub(r'\\bd\\s+', '', order) #! for d removal\n",
    "            tokens = word_tokenize(order)\n",
    "            # tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "            tokens = [lem_word(word) for word in tokens]\n",
    "            # print(tokens)\n",
    "            j = 0\n",
    "            to_index_train=training_data[i][k:]\n",
    "            if 'pizzaorder' in tokens:\n",
    "                tokens.remove('pizzaorder')\n",
    "                for word in to_index_train:\n",
    "                    if j == len(tokens):\n",
    "                        break\n",
    "                    if word == tokens[j]:\n",
    "                        order_category_labels[i][k] = 0\n",
    "                        j += 1\n",
    "                    k += 1\n",
    "            elif 'drinkorder' in tokens:\n",
    "                tokens.remove('drinkorder')\n",
    "                for word in to_index_train:\n",
    "                    if j == len(tokens):\n",
    "                        break\n",
    "                    if word == tokens[j]:\n",
    "                        order_category_labels[i][k] = 1\n",
    "                        j += 1\n",
    "                    k += 1\n",
    "    return order_category_labels\n",
    "\n",
    "order_category_labels = get_order_category_labels(training_top, training_data, stopwords)\n",
    "print(order_category_labels[:5])\n",
    "    \n",
    "    \n",
    "# def label_training_data(training_data, train_top):\n",
    "#     labeled_data = []\n",
    "    \n",
    "#     for sentence_tokens, top in zip(training_data, train_top):\n",
    "#         # Initialize labels for the sentence as \"None\"\n",
    "#         labels = [None] * len(sentence_tokens)\n",
    "\n",
    "#         # Regex to extract components from train.TOP\n",
    "#         component_regex = r\"\\((NUMBER|SIZE|STYLE|TOPPING|COMPLEX_TOPPING|QUANTITY)\\s+(.*?)\\)\"\n",
    "#         components = re.findall(component_regex, top)\n",
    "\n",
    "#         for component in components:\n",
    "#             key = component[0].upper()\n",
    "#             value = component[1].lower()\n",
    "\n",
    "#             # Tokenize the value from train.TOP\n",
    "#             value_tokens = word_tokenize(value)\n",
    "#             print(value_tokens)\n",
    "#             # Attempt to match the value tokens in the sentence tokens\n",
    "#             for i in range(len(sentence_tokens)):\n",
    "#                 # Match tokens sequentially\n",
    "#                 if sentence_tokens[i:i + len(value_tokens)] == value_tokens:\n",
    "#                     # Assign the label for each matching token\n",
    "#                     for j in range(len(value_tokens)):\n",
    "#                         if key == \"SIZE\":\n",
    "#                             labels[i + j] = \"SIZE\"\n",
    "#                         elif key == \"STYLE\":\n",
    "#                             labels[i + j] = \"STYLE\"\n",
    "#                         elif key == \"TOPPING\":\n",
    "#                             labels[i + j] = \"Topping\"\n",
    "#                         elif key == \"NUMBER\":\n",
    "#                             labels[i + j] = \"Number\"\n",
    "#                         elif key == \"COMPLEX_TOPPING\":\n",
    "#                             labels[i + j] = \"Complex-Topping\"\n",
    "#                         elif key == \"QUANTITY\":\n",
    "#                             labels[i + j] = \"Quantity\"\n",
    "\n",
    "#         # Append the labeled tokens for this sentence\n",
    "#         labeled_data.append((sentence_tokens, labels))\n",
    "\n",
    "#     return labeled_data\n",
    "\n",
    "# # Get labeled data\n",
    "# labeled_data_order_details = label_training_data(training_data, training_top)\n",
    "\n",
    "# # Display the results\n",
    "# for tokens, labels in labeled_data_order_details:\n",
    "#     print(\"Tokens:\", tokens)\n",
    "#     print(\"Labels:\", labels)\n",
    "#     print()\n",
    "\n",
    "\n",
    "\n",
    " # loop over order category_labels and make each consecutive 0 -> first 0 -> SOP, last 0 -> EOP ,each consecutive 1 -> first 1 -> SOD, last 1 -> EOD\n",
    " # in between 0,1 or any 2 -> None\n",
    "# def get_start_end_labels(order_category_labels):\n",
    "#     order_labels = []\n",
    "#     for item in order_category_labels:\n",
    "#         order_labels.append([4] * len(item))\n",
    "#         for i, label in enumerate(item):\n",
    "#             if label == 0:\n",
    "#                 if i == 0 or item[i - 1] != 0:\n",
    "#                     order_labels[-1][i] = 0\n",
    "#                 if i == len(item) - 1 or item[i + 1] != 0:\n",
    "#                     order_labels[-1][i] = 1\n",
    "#             elif label == 1:\n",
    "#                 if i == 0 or item[i - 1] != 1:\n",
    "#                     order_labels[-1][i] = 2\n",
    "#                 if i == len(item) - 1 or item[i + 1] != 1:\n",
    "#                     order_labels[-1][i] = 3\n",
    "#     return order_labels\n",
    "\n",
    "# order_category_labels=get_start_end_labels(order_category_labels)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# def process_training_top(training_top, stopwords):\n",
    "#     labeled_data = []\n",
    "\n",
    "#     def remove_unmatched_parentheses(text):\n",
    "#         stack = []\n",
    "#         result = list(text)\n",
    "#         for i, char in enumerate(text):\n",
    "#             if char == '(':\n",
    "#                 stack.append(i)\n",
    "#             elif char == ')':\n",
    "#                 if stack:\n",
    "#                     stack.pop()\n",
    "#                 else:\n",
    "#                     result[i] = ''  # Remove unmatched closing parenthesis\n",
    "#         for i in stack:  # Remove unmatched opening parentheses\n",
    "#             result[i] = ''\n",
    "#         return ''.join(result)\n",
    "\n",
    "#     for i, item in enumerate(training_top):\n",
    "#         sentence_tokens = []\n",
    "#         labels = []\n",
    "        \n",
    "#         # Remove unwanted keywords like PIZZAORDER or DRINKORDER\n",
    "#         unwanted_keywords = r'\\(\\b(ORDER|PIZZAORDER|DRINKORDER)\\b'\n",
    "#         cleaned_string = re.sub(unwanted_keywords, \"\", item)\n",
    "#         # Remove stopwords and unnecessary parentheses\n",
    "#         words = cleaned_string.split()\n",
    "#         cleaned_words = [word for word in words if word.lower() not in stopwords]\n",
    "#         cleaned_string = ' '.join(cleaned_words)\n",
    "        \n",
    "#         # Remove unmatched parentheses\n",
    "#         cleaned_string = remove_unmatched_parentheses(cleaned_string)\n",
    "\n",
    "#         # Regex to match nested parentheses\n",
    "#         pattern_regex = r\"\\((\\w+)\\s+((?:[^\\(\\)]|\\([^()]*\\))*)\\)\"\n",
    "#         matches = list(re.finditer(pattern_regex, cleaned_string))\n",
    "        \n",
    "#         in_not_block = False\n",
    "\n",
    "#         for idx, match in enumerate(matches):\n",
    "#             key, value = match.groups()\n",
    "#             # Handle NOT block\n",
    "#             if key == \"NOT\":\n",
    "#                 in_not_block = True\n",
    "#                 # Match nested content within the NOT block\n",
    "#                 nested_match = re.match(pattern_regex, value.strip())\n",
    "#                 if nested_match:\n",
    "#                     nested_key, nested_value = nested_match.groups()\n",
    "#                     print(f\"Nested Match - Key: {nested_key}, Value: {nested_value}\")\n",
    "#                     value_tokens = word_tokenize(nested_value.strip())\n",
    "#                     sentence_tokens.extend(value_tokens)\n",
    "#                     labels.extend([f\"NOT_{nested_key}\"] * len(value_tokens))\n",
    "#                 in_not_block = False  # Reset NOT block\n",
    "#                 continue\n",
    "\n",
    "#             value = re.sub(r'\\(\\s*\\w+\\s*', '', value)  # Match '(WORD)'\n",
    "#             value = re.sub(r'[()]', '', value)  # Remove any remaining parentheses\n",
    "#             # Tokenize the value\n",
    "#             value_tokens = word_tokenize(value.strip())\n",
    "#             sentence_tokens.extend(value_tokens)\n",
    "#             # Labeling logic\n",
    "#             if key == \"STYLE\":\n",
    "#                 labels.extend([\"STYLE\"] * len(value_tokens))\n",
    "#             elif key == \"SIZE\":\n",
    "#                 labels.extend([\"SIZE\"] * len(value_tokens))\n",
    "#             elif key == \"TOPPING\":\n",
    "#                 labels.extend([\"Topping\"] * len(value_tokens))\n",
    "#             elif key == \"NUMBER\":\n",
    "#                 labels.extend([\"Number\"] * len(value_tokens))\n",
    "#             elif key == \"COMPLEX_TOPPING\":\n",
    "#                 labels.extend([\"Complex-Topping\"] * len(value_tokens))\n",
    "#             elif key == \"QUANTITY\":\n",
    "#                 labels.extend([\"Complex-Topping\"] * len(value_tokens))\n",
    "#             else:\n",
    "#                 labels.extend([\"None\"] * len(value_tokens))\n",
    "\n",
    "#         # Add the processed tokens and labels for the current sentence\n",
    "#         if sentence_tokens:\n",
    "#             labeled_data.append((sentence_tokens, labels))\n",
    "\n",
    "#     return labeled_data\n",
    "\n",
    "# labeled_data = process_training_top(training_top, stopwords)\n",
    "\n",
    "# # Output the labeled data\n",
    "# for tokens, labels in labeled_data:\n",
    "#     print(\"Tokens:\", tokens)\n",
    "#     print(\"Labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = (\n",
    "    'train_order_category_labels_1.txt' if datatype == 'train' else\n",
    "    'dev_order_category_labels_1.txt' if datatype == 'dev' else\n",
    "    'dev_order_category_labels.txt' if datatype == 'test' else\n",
    "    'unknown_order_category_labels.txt'  # Optional fallback\n",
    ")\n",
    "for labels in order_category_labels:\n",
    "    with open(path, 'a') as f: # dev\n",
    "        f.write(\"%s\\n\" % labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
