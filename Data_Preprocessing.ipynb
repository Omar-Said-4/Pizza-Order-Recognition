{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clean-text in c:\\program files\\python311\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in c:\\program files\\python311\\lib\\site-packages (from clean-text) (1.7.0)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in c:\\program files\\python311\\lib\\site-packages (from clean-text) (6.3.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\omara\\appdata\\roaming\\python\\python311\\site-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.8)\n",
      "Requirement already satisfied: inflect in c:\\program files\\python311\\lib\\site-packages (7.4.0)\n",
      "Requirement already satisfied: more-itertools>=8.5.0 in c:\\users\\omara\\appdata\\roaming\\python\\python311\\site-packages (from inflect) (10.3.0)\n",
      "Requirement already satisfied: typeguard>=4.0.1 in c:\\program files\\python311\\lib\\site-packages (from inflect) (4.4.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\program files\\python311\\lib\\site-packages (from typeguard>=4.0.1->inflect) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install clean-text\n",
    "!pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\omara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\omara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import inflect\n",
    "from cleantext import clean\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: PIZZA_train.json\n",
      "Lines: 50000\n"
     ]
    }
   ],
   "source": [
    "datatype = 'train' \n",
    "\n",
    "path = (\n",
    "    'PIZZA_train.json' if datatype == 'train' else\n",
    "    'PIZZA_dev.json' if datatype == 'dev' else\n",
    "    'PIZZA_test.json' if datatype == 'test' else\n",
    "    'unknown.json'  # Optional fallback\n",
    ")\n",
    "\n",
    "lines = (\n",
    "    50000 if datatype == 'train' else\n",
    "    847 if datatype == 'dev' else\n",
    "    1357 if datatype == 'test' else\n",
    "    0\n",
    ")\n",
    "\n",
    "print(f\"Path: {path}\")\n",
    "print(f\"Lines: {lines}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#! load data given file path and number of line to load\n",
    "def load_data(data, file_path,num_lines):\n",
    "    # data = []\n",
    "    with open(file_path, 'r') as file: \n",
    "        for _ in range(num_lines):\n",
    "            data.append(json.loads(file.readline()))\n",
    "            # i+=1\n",
    "    #         if i%100000==0 or (i>2450000):\n",
    "    #             print(i) \n",
    "    # print(f\"Loaded {num_lines} lines\")\n",
    "    # return data\n",
    "# def load_random_data(file_path, num_lines, last_lines):\n",
    "#     data = []\n",
    "    \n",
    "#     with open(file_path, 'r') as file:\n",
    "#         file.seek(0, 2)  # Move to the end of the file to get its size\n",
    "#         file_size = file.tell()\n",
    "        \n",
    "        # Read the last `last_lines` lines\n",
    "        file.seek(max(file_size - 1024 * last_lines, 0))  # Optional: Adjust the seek window size for efficiency\n",
    "        lines = file.readlines()\n",
    "        \n",
    "        # Parse last lines with error handling\n",
    "        last_lines_data = []\n",
    "        for line in lines[-last_lines:]:\n",
    "            if line.strip():\n",
    "                try:\n",
    "                    last_lines_data.append(json.loads(line.strip()))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Skipping invalid JSON line: {line.strip()} - {e}\")\n",
    "        \n",
    "        data.extend(last_lines_data)\n",
    "\n",
    "        # Calculate size of remaining lines\n",
    "        remaining_lines_size = file_size - sum(len(line) for line in lines[-last_lines:])\n",
    "\n",
    "        # Read `num_lines` random lines from the rest of the file (excluding the last `last_lines` lines)\n",
    "        for _ in range(num_lines):\n",
    "            while True:\n",
    "                try:\n",
    "                    # Pick a random position outside the last `last_lines` range\n",
    "                    random_pos = random.randint(0, remaining_lines_size - 1)\n",
    "                    file.seek(random_pos)\n",
    "                    \n",
    "                    # Read to the end of the current line to avoid partial lines\n",
    "                    file.readline()  # Skip partial line\n",
    "                    line = file.readline()  # Read the next complete line\n",
    "\n",
    "                    if line.strip():  # Ensure it's not empty\n",
    "                        data.append(json.loads(line.strip()))\n",
    "                        break  # Exit loop when a valid line is added\n",
    "                except json.JSONDecodeError as e:\n",
    "                    # Skip invalid JSON and continue attempting to fetch another line\n",
    "                    continue  \n",
    "                except Exception as e:\n",
    "                    # Catch any unexpected exceptions\n",
    "                    print(f\"Error during random data load: {e}\")\n",
    "                    break\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'train.SRC': \"i'd like a pizza with red onion fried onions and mozarella without thin crust\", 'train.EXR': '(ORDER (PIZZAORDER (NUMBER 1 ) (TOPPING RED_ONIONS ) (TOPPING FRIED_ONIONS ) (TOPPING MOZZARELLA_CHEESE ) (NOT (STYLE THIN_CRUST ) ) ) )', 'train.TOP': \"(ORDER i'd like (PIZZAORDER (NUMBER a ) pizza with (TOPPING red onion ) (TOPPING fried onions ) and (TOPPING mozarella ) without (NOT (STYLE thin crust ) ) ) )\", 'train.TOP-DECOUPLED': '(ORDER (PIZZAORDER (NUMBER a ) (TOPPING red onion ) (TOPPING fried onions ) (TOPPING mozarella ) (NOT (STYLE thin crust ) ) ) )'}, {'train.SRC': \"i'd like a pizza with anchovy caramelized red onion and roasted green pepper without thin crust\", 'train.EXR': '(ORDER (PIZZAORDER (NUMBER 1 ) (TOPPING ANCHOVIES ) (TOPPING CARAMELIZED_ONIONS ) (TOPPING ROASTED_GREEN_PEPPERS ) (NOT (STYLE THIN_CRUST ) ) ) )', 'train.TOP': \"(ORDER i'd like (PIZZAORDER (NUMBER a ) pizza with (TOPPING anchovy ) (TOPPING caramelized red onion ) and (TOPPING roasted green pepper ) without (NOT (STYLE thin crust ) ) ) )\", 'train.TOP-DECOUPLED': '(ORDER (PIZZAORDER (NUMBER a ) (TOPPING anchovy ) (TOPPING caramelized red onion ) (TOPPING roasted green pepper ) (NOT (STYLE thin crust ) ) ) )'}, {'train.SRC': \"i'd like a pizza with applewood bacon grilled pineapple and shrimps without thin crust\", 'train.EXR': '(ORDER (PIZZAORDER (NUMBER 1 ) (TOPPING BACON ) (TOPPING GRILLED_PINEAPPLE ) (TOPPING SHRIMPS ) (NOT (STYLE THIN_CRUST ) ) ) )', 'train.TOP': \"(ORDER i'd like (PIZZAORDER (NUMBER a ) pizza with (TOPPING applewood bacon ) (TOPPING grilled pineapple ) and (TOPPING shrimps ) without (NOT (STYLE thin crust ) ) ) )\", 'train.TOP-DECOUPLED': '(ORDER (PIZZAORDER (NUMBER a ) (TOPPING applewood bacon ) (TOPPING grilled pineapple ) (TOPPING shrimps ) (NOT (STYLE thin crust ) ) ) )'}]\n"
     ]
    }
   ],
   "source": [
    "data=load_random_data(path,lines,1000) if datatype=='train' else load_data(path,lines)\n",
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i'd like a pizza with red onion fried onions and mozarella without thin crust\", \"i'd like a pizza with anchovy caramelized red onion and roasted green pepper without thin crust\"]\n",
      "['(ORDER (PIZZAORDER (NUMBER 1 ) (TOPPING RED_ONIONS ) (TOPPING FRIED_ONIONS ) (TOPPING MOZZARELLA_CHEESE ) (NOT (STYLE THIN_CRUST ) ) ) )', '(ORDER (PIZZAORDER (NUMBER 1 ) (TOPPING ANCHOVIES ) (TOPPING CARAMELIZED_ONIONS ) (TOPPING ROASTED_GREEN_PEPPERS ) (NOT (STYLE THIN_CRUST ) ) ) )']\n",
      "[\"(ORDER i'd like (PIZZAORDER (NUMBER a ) pizza with (TOPPING red onion ) (TOPPING fried onions ) and (TOPPING mozarella ) without (NOT (STYLE thin crust ) ) ) )\", \"(ORDER i'd like (PIZZAORDER (NUMBER a ) pizza with (TOPPING anchovy ) (TOPPING caramelized red onion ) and (TOPPING roasted green pepper ) without (NOT (STYLE thin crust ) ) ) )\"]\n"
     ]
    }
   ],
   "source": [
    "#! split json data\n",
    "def getextensions(datatype):\n",
    "    if datatype=='train':\n",
    "        return 'train.SRC','train.EXR','train.TOP'\n",
    "    elif datatype=='dev':\n",
    "        return 'dev.SRC','dev.EXR','dev.TOP'\n",
    "    else:\n",
    "        return 'test.SRC','test.EXR','test.TOP'\n",
    "def get_training_data(data,datatype='train'):\n",
    "    values=getextensions(datatype)\n",
    "    training_data = []\n",
    "    training_exr=[]\n",
    "    training_top=[]\n",
    "    # training_top_dec=[]\n",
    "    for item in data:\n",
    "        training_data.append(item[values[0]])  \n",
    "        training_exr.append(item[values[1]])  \n",
    "        training_top.append(item[values[2]])\n",
    "        # if datatype=='train':\n",
    "        #     training_top_dec.append(item[values[3]])  \n",
    "    return training_data,training_exr,training_top\n",
    "training_data,training_exr,training_top=get_training_data(data,datatype)   \n",
    "print(training_data[:2])\n",
    "print(training_exr[:2])\n",
    "print(training_top[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i need one large pizza and a bit olives and extra cheese as well as chicken on it thanks a lot\n"
     ]
    }
   ],
   "source": [
    "# Create an inflect engine\n",
    "p = inflect.engine()\n",
    "\n",
    "def replace_numbers_with_words(text):\n",
    "    # Regular expression to find numbers in the text\n",
    "    number_pattern = r'\\b\\d+\\b'\n",
    "    \n",
    "    # Function to convert a number match to its word representation\n",
    "    def number_to_words(match):\n",
    "        number = int(match.group())\n",
    "        # Convert to words and replace hyphens with underscores\n",
    "        return p.number_to_words(number, andword=\"\").replace('-', '_')\n",
    "\n",
    "    \n",
    "    # Substitute numbers with their word representation\n",
    "    text = re.sub(number_pattern, number_to_words, text)\n",
    "    \n",
    "    # Replace standalone 'a' or 'an' with 'one' where appropriate\n",
    "    text = re.sub(\n",
    "        r'\\b(a|an)\\b(?! ((lot)|(little)|(few)|(number)|(couple)|(bit)|(load)|(stack)|(bunch)|(group)|(set)|(series)|(variety)|(range)|(amount)|(sum)|(total)))',\n",
    "       'one',\n",
    "        text,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "print(replace_numbers_with_words(\"i need a large pizza and a bit olives and extra cheese as well as chicken on it thanks a lot\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! a function to handle negations\n",
    "def handle_negations(text):\n",
    "    #! negations are based on training set\n",
    "    negations_pattern = r\"\\b(?:no|not|without)\\s+.*?\\b(?=(?:[^\\w\\s]|$))\"\n",
    "    # print(re.findall(negations_pattern, text))\n",
    "    text = re.sub(negations_pattern, lambda x: ' '.join([f'not_{word}' for word in x.group(0).split()]), text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!# lemmatize words with all possible pos tags\n",
    "def lem_word(word):\n",
    "    if(word=='bit'):\n",
    "        return word\n",
    "    possible_pos = [wordnet.NOUN, wordnet.VERB, wordnet.ADJ, wordnet.ADV]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for pos in possible_pos:\n",
    "        word=lemmatizer.lemmatize(word,pos)\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "#! stopwords list (adding not_)\n",
    "stopwords = set(stopwords.words('english'))\n",
    "stopwords.add('like')\n",
    "not_stopwords = ['not_' + word for word in stopwords]\n",
    "# stopwords.update(not_stopwords)\n",
    "# stopwords.discard('a')\n",
    "# stopwords.discard('an')\n",
    "# stopwords.discard('not')\n",
    "# stopwords.discard('no')\n",
    "# stopwords.discard('can')\n",
    "# stopwords.discard('not_a')\n",
    "# stopwords.discard('not_an')\n",
    "# stopwords.discard('not_can')\n",
    "# stopwords.discard('not_no')\n",
    "stopwords=set()\n",
    "# stopwords.add('and')\n",
    "# stopwords.add('also')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! tokenize training data and remove stop words\n",
    "def preprocess_training_data(training_data, stopwords):\n",
    "    # training_data = [handle_negations(order) for order in training_data]\n",
    "    training_data=[replace_numbers_with_words(order) for order in training_data]\n",
    "    training_data = [word_tokenize(order) for order in training_data]\n",
    "    # print(training_data)\n",
    "    training_data = [[word.lower() for word in order if word.lower() not in stopwords] for order in training_data]\n",
    "    training_data = [clean(order, no_line_breaks=True, no_punct=True, no_currency_symbols=True) for order in training_data]\n",
    "    # print(training_data)\n",
    "    # print(training_data)\n",
    "    #! remove d letter most probably garbage\n",
    "    training_data = [re.sub(r'\\bd\\s+', '', order) for order in training_data]\n",
    "    #! remove \"can\" at the beginning of the sentence\n",
    "    # training_data=[re.sub(r'^can\\s+', '', order) for order in training_data]\n",
    "    training_data = [word_tokenize(order) for order in training_data]\n",
    "    # training_data = [[lemmatizer.lemmatize(word) for word in order] for order in training_data]\n",
    "    training_data = [[lem_word(word) for word in order] for order in training_data]\n",
    "    return training_data\n",
    "training_data = preprocess_training_data(training_data, stopwords)\n",
    "# print(training_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! save processed training dataset\n",
    "path = (\n",
    "    'trian_data_order_category.txt' if datatype == 'train' else\n",
    "    'dev_data_order_category.txt'\n",
    ")\n",
    "with open(path, 'a') as f: #### dev_data_processed.txt\n",
    "    for item in training_data:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! a utility function for extra parentheses ) removal \n",
    "#! handles COMPLEX_TOPPING, NOT,... parenthesis cases\n",
    "def remove_unmatched_parentheses(input_string):\n",
    "    result = list(input_string)  # Convert to list for mutability\n",
    "    last_bracket_index=-1\n",
    "    for i, char in enumerate(result):\n",
    "        if char == ')' and i+2 < len(result):\n",
    "            result[i] = ''  \n",
    "            last_bracket_index=i\n",
    "        elif char == '(':\n",
    "            if last_bracket_index!=-1:\n",
    "                result[last_bracket_index] = ')'\n",
    "                last_bracket_index=-1\n",
    "        elif char == ')' and i+2 >= len(result):\n",
    "            result[i] = ''\n",
    "            result[last_bracket_index] = ')'\n",
    "    return ''.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! get PIZZAORDER, DRINKORDER, NONE Labels 0=>PIZZAORDER, 1=>DRINKORDER, 2=>NONE\n",
    "\n",
    "def get_order_category_labels(training_top, training_data, stopwords):\n",
    "    # print(training_data)\n",
    "    order_category_labels = []\n",
    "    for i, item in enumerate(training_top):\n",
    "        order_category_labels.append([2] * len(training_data[i]))\n",
    "        #! remove any extra labelling\n",
    "        item = re.sub(r'\\)[^()]*\\(', ') (', item)\n",
    "        # print(item)\n",
    "        unwanted_keywords = r\"\\b(ORDER|SIZE|STYLE|TOPPING|COMPLEX_TOPPING|QUANTITY|NOT|NUMBER|DRINKTYPE|CONTAINERTYPE|VOLUME)\\b\"\n",
    "        cleaned_string = re.sub('\\('+unwanted_keywords, \"\", item)\n",
    "        cleaned_string = [word for word in cleaned_string.split() if word.lower() not in stopwords]\n",
    "        cleaned_string = ' '.join(cleaned_string)\n",
    "        cleaned_string = remove_unmatched_parentheses(cleaned_string)\n",
    "        order_regex = r\"\\((?:PIZZAORDER|DRINKORDER).*?\\)\"\n",
    "        extracted_orders = re.findall(order_regex, cleaned_string)\n",
    "        k = 0\n",
    "        for order in extracted_orders:\n",
    "            order=replace_numbers_with_words(order)\n",
    "            order = re.sub(r\"[\\(\\)]\", \"\", order)\n",
    "            order=word_tokenize(order) #! fix id and don't bugs\n",
    "            order=clean(order, no_line_breaks=True, no_punct=True, no_currency_symbols=True)\n",
    "            order=re.sub(r'\\bd\\s+', '', order) #! for d removal\n",
    "            tokens = word_tokenize(order)\n",
    "            # tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "            tokens = [lem_word(word) for word in tokens]\n",
    "            j = 0\n",
    "            to_index_train=training_data[i][k:]\n",
    "            if 'pizzaorder' in tokens:\n",
    "                tokens.remove('pizzaorder')\n",
    "                for word in to_index_train:\n",
    "                    if j == len(tokens):\n",
    "                        break\n",
    "                    if word == tokens[j]:\n",
    "                        order_category_labels[i][k] = 0\n",
    "                        j += 1\n",
    "                    k += 1\n",
    "            elif 'drinkorder' in tokens:\n",
    "                tokens.remove('drinkorder')\n",
    "                for word in to_index_train:\n",
    "                    if j == len(tokens):\n",
    "                        break\n",
    "                    if word == tokens[j]:\n",
    "                        order_category_labels[i][k] = 1\n",
    "                        j += 1\n",
    "                    k += 1\n",
    "    return order_category_labels\n",
    "\n",
    "order_category_labels = get_order_category_labels(training_top, training_data, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = (\n",
    "    'train_labels_order_category.txt' if datatype == 'train' else\n",
    "    'dev_labels_order_category.txt'\n",
    ")\n",
    "for labels in order_category_labels:\n",
    "    with open(path, 'a') as f: # dev\n",
    "        f.write(\"%s\\n\" % labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'want', 'to', 'have', 'one', 'small', 'pizza', 'without', 'tuna', 'but', 'with', 'olive', 'and', 'extra', 'cheese'], ['get', 'me', 'one', 'large', 'tuna', 'pie', 'with', 'pepperoni', 'and', 'olive']]\n",
      "[[2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[['i', 'want', 'to', 'have', 'one', 'small', 'pizza', 'without', 'tuna', 'but', 'with', 'olive', 'and', 'extra', 'cheese'], ['get', 'me', 'one', 'large', 'tuna', 'pie', 'with', 'pepperoni', 'and', 'olive']]\n",
      "[[0, 0, 0, 0, 1, 2, 0, 0, 6, 0, 0, 4, 0, 5, 4], [0, 0, 1, 2, 4, 0, 0, 4, 0, 4]]\n"
     ]
    }
   ],
   "source": [
    "def remove_unmatched_parentheses(text):\n",
    "    stack = []\n",
    "    result = list(text)\n",
    "    for i, char in enumerate(text):\n",
    "        if char == '(':\n",
    "            stack.append(i)\n",
    "        elif char == ')':\n",
    "            if stack:\n",
    "                stack.pop()\n",
    "            else:\n",
    "                result[i] = ''  # Remove unmatched closing parenthesis\n",
    "    for i in stack:  # Remove unmatched opening parentheses\n",
    "        result[i] = ''\n",
    "    return ''.join(result)\n",
    "\n",
    "def process_training_top(training_top, stopwords):\n",
    "    labeled_data = []\n",
    "    for i, item in enumerate(training_top):\n",
    "        sentence_tokens = []\n",
    "        labels = []\n",
    "        # Remove unwanted keywords like PIZZAORDER or DRINKORDER\n",
    "        unwanted_keywords = r'\\(\\b(ORDER|PIZZAORDER|DRINKORDER)\\b'\n",
    "        cleaned_string = re.sub(unwanted_keywords, \"\", item)\n",
    "        # Remove stopwords and unnecessary parentheses\n",
    "        words = cleaned_string.split()\n",
    "        cleaned_words = [word for word in words if word.lower() not in stopwords]\n",
    "        cleaned_string = ' '.join(cleaned_words)\n",
    "\n",
    "        # Remove unmatched parentheses\n",
    "        cleaned_string = remove_unmatched_parentheses(cleaned_string)\n",
    "        # Regex to match nested parentheses excluding COMPLEX_TOPPING\n",
    "        pattern_regex = r\"\\((?!COMPLEX_TOPPING\\b)(\\w+)\\s+((?:[^\\(\\)]|\\([^()]*\\))*)\\)\"\n",
    "        matches = list(re.finditer(pattern_regex, cleaned_string))\n",
    "        for _, match in enumerate(matches):\n",
    "            key, value = match.groups()\n",
    "            # Handle NOT block\n",
    "            if key == \"NOT\":\n",
    "                nested_match = re.match(pattern_regex, value.strip())\n",
    "                if nested_match:\n",
    "                    nested_key, nested_value = nested_match.groups()\n",
    "                    nested_value=replace_numbers_with_words(nested_value)\n",
    "                    nested_value = clean(nested_value, no_line_breaks=True, no_punct=True, no_currency_symbols=True)\n",
    "                    nested_value = re.sub(r'\\bd\\s+', '', nested_value)  # Remove 'd'\n",
    "                    value_tokens = word_tokenize(nested_value.strip())\n",
    "                    value_tokens = [lem_word(word) for word in value_tokens]\n",
    "                    sentence_tokens.extend(value_tokens)\n",
    "                    labels.extend([f\"NOT_{nested_key}\"] * len(value_tokens))\n",
    "                continue\n",
    "\n",
    "            value = re.sub(r'\\(\\s*\\w+\\s*', '', value)  # Match '(WORD)' for complex toppings\n",
    "            value = re.sub(r'[()]', '', value)  # Remove any remaining parentheses\n",
    "            value_tokens = replace_numbers_with_words(value.strip())\n",
    "            value_tokens = clean(value_tokens, no_line_breaks=True, no_punct=True, no_currency_symbols=True)\n",
    "            value_tokens = re.sub(r'\\bd\\s+', '', value_tokens)  # Remove 'd'\n",
    "            value_tokens = word_tokenize(value_tokens)\n",
    "            value_tokens = [lem_word(word) for word in value_tokens]\n",
    "            sentence_tokens.extend(value_tokens)\n",
    "\n",
    "            # Labeling logic\n",
    "            label_mapping = {\n",
    "                \"STYLE\": \"STYLE\",\n",
    "                \"SIZE\": \"SIZE\",\n",
    "                \"TOPPING\": \"Topping\",\n",
    "                \"NUMBER\": \"Number\",\n",
    "                \"QUANTITY\": \"Complex-Topping\",\n",
    "                \"DRINKTYPE\": \"Drinktype\",\n",
    "                \"CONTAINERTYPE\": \"ContainerType\",\n",
    "                \"VOLUME\":\"Volume\"\n",
    "            }\n",
    "            labels.extend([label_mapping.get(key, \"None\")] * len(value_tokens))\n",
    "        if sentence_tokens:\n",
    "            labeled_data.append((sentence_tokens, labels))\n",
    "\n",
    "    return labeled_data\n",
    "\n",
    "def update_training_labels(training_data, training_labels, labeled_data,training_top):\n",
    "    # Define a mapping for new labels\n",
    "    label_mapping = {\n",
    "    \"None\": 0,\n",
    "    \"Number\": 1,\n",
    "    \"SIZE\": 2,\n",
    "    \"STYLE\": 3,\n",
    "    \"Topping\": 4,\n",
    "    \"Complex-Topping\": 5,\n",
    "    \"NOT_TOPPING\": 6,\n",
    "    \"NOT_STYLE\": 7,\n",
    "    \"Drinktype\": 8,\n",
    "    \"ContainerType\": 9,\n",
    "    \"Volume\": 10,\n",
    "    }\n",
    "    # Convert data and labels into modifiable lists\n",
    "    new_training_data = [list(tokens) for tokens in training_data]\n",
    "    new_training_labels = [[0 for _ in labels] for labels in training_labels]\n",
    "    for idx, (train_tokens, train_labels) in enumerate(zip(new_training_data, new_training_labels)):\n",
    "        labeled_tokens, labeled_labels = labeled_data[idx]  # Get labeled tokens and their labels\n",
    "        # Initialize pointers\n",
    "        train_ptr = 0\n",
    "        labeled_ptr = 0\n",
    "        \n",
    "        while train_ptr < len(train_tokens) and labeled_ptr < len(labeled_tokens):\n",
    "            if train_tokens[train_ptr] == labeled_tokens[labeled_ptr]:\n",
    "                # Map the label using the label_mapping dictionary\n",
    "                train_labels[train_ptr] = label_mapping.get(labeled_labels[labeled_ptr], label_mapping[\"None\"])\n",
    "                labeled_ptr += 1  # Move labeled_tokens pointer\n",
    "            train_ptr += 1  # Move training_tokens pointer\n",
    "        \n",
    "        # If the labeled_ptr hasn't reached the end, the remaining labeled tokens were not found\n",
    "        if labeled_ptr != len(labeled_tokens):\n",
    "            print(f\"Warning: Not all labeled tokens found in training tokens for index {idx}.\")\n",
    "            print(labeled_tokens)\n",
    "            print(train_tokens)\n",
    "            print(training_top[idx])\n",
    "    return new_training_data, new_training_labels\n",
    "labeled_data = process_training_top(training_top, stopwords)\n",
    "Model2_data, Model2_labels = update_training_labels(training_data, order_category_labels, labeled_data,training_top)\n",
    "print(training_data[168:170])\n",
    "print(order_category_labels[168:170]) \n",
    "print(Model2_data[168:170])\n",
    "print(Model2_labels[168:170])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = (\n",
    "    'train_data_order_details.txt' if datatype == 'train' else\n",
    "    'dev_data_order_details.txt'\n",
    ")\n",
    "\n",
    "with open(path, 'a') as f: \n",
    "    for item in Model2_data:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = (\n",
    "    'train_labels_order_details.txt' if datatype == 'train' else\n",
    "    'dev_labels_order_details.txt'\n",
    ")\n",
    "for labels in Model2_labels:\n",
    "    with open(path, 'a') as f:\n",
    "        f.write(\"%s\\n\" % labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
