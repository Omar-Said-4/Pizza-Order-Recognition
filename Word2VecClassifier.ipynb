{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "import gensim as gs\n",
    "import ast\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! check if tensorflow is using GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(f\"TensorFlow is using {len(gpus)} GPU(s).\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"GPU: {gpu.name}\")\n",
    "else:\n",
    "    print(\"TensorFlow is not using any GPUs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, vectorize and pad the training data with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_labels(data_path, labels_path):\n",
    "    with open(data_path, 'r') as f:\n",
    "        data = [ast.literal_eval(line.strip()) for line in f.readlines()]\n",
    "    with open(labels_path, 'r') as f:\n",
    "        labels = [ast.literal_eval(line.strip()) for line in f.readlines()]\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,labels=load_data_labels('training_data_processed.txt','order_category_labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[:5])\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gs.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! add the unknown token to the vocabulary\n",
    "unk_token = \"UNK\"\n",
    "for sentence in data:\n",
    "    for i, word in enumerate(sentence):\n",
    "        if word not in model and word != 'a':\n",
    "            sentence[i] = unk_token\n",
    "#! extract the vocabulary from the data\n",
    "vocab = set()  \n",
    "for sentence in data:\n",
    "    vocab.update(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! get the embedings matrix\n",
    "embedding_dim = 300  \n",
    "embedding_matrix = np.random.randn(len(vocab), embedding_dim)\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "for word, idx in word_to_index.items():\n",
    "    if word in model:\n",
    "        embedding_matrix[idx] = model[word]  #! Use pre-trained embedding\n",
    "    elif word =='a':\n",
    "        embedding_matrix[idx] = model['one']\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.zeros(embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 100  #! Maximum sequence length after padding\n",
    "input_dim = len(vocab)  # Vocabulary size\n",
    "embedding_dim = 300  # Embedding dimension\n",
    "output_dim = 3  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! convert words to indices, make all the sentences of the same length by padding them with non used value -1\n",
    "sequences = [[word_to_index[word] for word in text] for text in data]\n",
    "X = pad_sequences(sequences, padding='post',value=-1,maxlen=max_sequence_length)\n",
    "#! make all the labels of the same length by padding them with none value ==> 2\n",
    "Y = pad_sequences(labels,padding='post',value=2,maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! model\n",
    "Bidirectional_LSTM_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Masking(mask_value=-1),  #! Masking layer to handle the padded values\n",
    "    tf.keras.layers.Embedding(input_dim=input_dim, output_dim=embedding_dim, weights=[embedding_matrix], trainable=True,mask_zero=False),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
    "    tf.keras.layers.Dropout(0.6),\n",
    "    tf.keras.layers.Dense(output_dim, activation='softmax')\n",
    "])\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss',       #! Monitor the validation loss (you can use 'accuracy' or another metric)\n",
    "    factor=0.5,               #! Factor by which the learning rate will be reduced\n",
    "    patience=5,               #! Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=1e-9,              #! Lower bound on the learning rate\n",
    "    verbose=1                 #! Print a message when the learning rate is reduced\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "Bidirectional_LSTM_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bidirectional_LSTM_model.fit(X, Y, epochs=2, batch_size=1024, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bidirectional_LSTM_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train=Bidirectional_LSTM_model.predict(X[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train=np.argmax(preds_train,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for i in range(100000):\n",
    "    mask=X[i]!=-1\n",
    "    if np.array_equal(preds_train[i][mask],Y[i][mask]):\n",
    "        count+=1\n",
    "print(count/100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! load dev set and labels\n",
    "dev_data,dev_labels=load_data_labels('dev_data_processed.txt','dev_order_category_labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! replace any words not in the vocabulary with the unknown token\n",
    "for sentence in dev_data:\n",
    "    for i, word in enumerate(sentence):\n",
    "        if word not in vocab:\n",
    "            sentence[i] = unk_token\n",
    "#! convert words to indices, make all the sentences of the same length by padding them with non used value -1\n",
    "sequences_d = [[word_to_index[word] for word in text] for text in dev_data]\n",
    "Xd = pad_sequences(sequences_d, padding='post',value=-1,maxlen=max_sequence_length)\n",
    "#! make all the labels of the same length by padding them with none value ==> 2\n",
    "Yd = pad_sequences(dev_labels,padding='post',value=2,maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_dev=Bidirectional_LSTM_model.predict(Xd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_dev=np.argmax(preds_dev,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for i in range(len(dev_data)):\n",
    "    mask=Xd[i]!=-1\n",
    "    if np.array_equal(preds_dev[i][mask],Yd[i][mask]):\n",
    "        count+=1\n",
    "    print(i)\n",
    "    print(preds_dev[i][mask])\n",
    "    print(Yd[i][mask])\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "print(count/len(dev_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
