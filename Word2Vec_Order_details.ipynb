{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Masking, Embedding, Bidirectional, LSTM, Attention, Dropout, Dense\n",
    "from tensorflow.keras import Model, Input\n",
    "import gensim as gs\n",
    "import ast\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#! check if tensorflow is using GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(f\"TensorFlow is using {len(gpus)} GPU(s).\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"GPU: {gpu.name}\")\n",
    "else:\n",
    "    print(\"TensorFlow is not using any GPUs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_labels(data_path, labels_path):\n",
    "    with open(data_path, 'r') as f:\n",
    "        data = [ast.literal_eval(line.strip()) for line in f]\n",
    "    with open(labels_path, 'r') as f:\n",
    "        labels = [ast.literal_eval(line.strip()) for line in f]\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data, labels = load_data_labels('training_data_processed.txt', 'train_order_category_labels.txt')\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "print(data[:5])\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dev_data, dev_labels = load_data_labels('dev_data_processed.txt', 'dev_order_category_labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_model = gs.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#! get v Aand replace unknown words with unk token\n",
    "def process_sentence(sentence, model):\n",
    "    for i, word in enumerate(sentence):\n",
    "        if word not in model and word  != 'a':\n",
    "            sentence[i] = 'unk'\n",
    "    return sentence\n",
    "\n",
    "data = [process_sentence(sentence, pretrained_model) for sentence in data]\n",
    "print(data[:5])\n",
    "vocab=set()\n",
    "for sentence in data:\n",
    "    vocab.update(sentence)\n",
    "#! get word index for each word in vocab\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim=300\n",
    "input_dim=len(vocab)\n",
    "output_dim=3\n",
    "max_length=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for tokens in dev_data:\n",
    "    for i,word in enumerate(tokens):\n",
    "        if word not in vocab!= 'a':\n",
    "            tokens[i] = 'unk'\n",
    "X_d=[[word2idx[word] for word in sentence] for sentence in dev_data]\n",
    "X_d=pad_sequences(X_d, maxlen=max_length, padding='post', value=-1)\n",
    "Y_d=pad_sequences(dev_labels, maxlen=max_length, padding='post', value=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#! get embeddings matrix\n",
    "def get_embeddings_matrix(model, vocab):\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_dim = model.vector_size\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for i, word in enumerate(vocab):\n",
    "        if word in model:\n",
    "            embedding_matrix[i] = model[word]\n",
    "        elif word == 'a':\n",
    "            embedding_matrix[i] = model['one']\n",
    "    return embedding_matrix\n",
    "embedding_matrix = get_embeddings_matrix(pretrained_model, vocab)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
