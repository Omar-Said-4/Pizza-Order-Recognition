{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from gensim) (1.24.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Masking, Embedding, Bidirectional, LSTM, Attention, Dropout, Dense\n",
    "from tensorflow.keras import Model, Input\n",
    "import gensim as gs\n",
    "import ast\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow is using 1 GPU(s).\n",
      "GPU: /physical_device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "#! check if tensorflow is using GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(f\"TensorFlow is using {len(gpus)} GPU(s).\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"GPU: {gpu.name}\")\n",
    "else:\n",
    "    print(\"TensorFlow is not using any GPUs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_labels(data_path, labels_path):\n",
    "    with open(data_path, 'r') as f:\n",
    "        data = [ast.literal_eval(line.strip()) for line in f]\n",
    "    with open(labels_path, 'r') as f:\n",
    "        labels = [ast.literal_eval(line.strip()) for line in f]\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51000\n",
      "51000\n",
      "[['i', 'like', 'one', 'pizza', 'with', 'red', 'onion', 'fry', 'onion', 'and', 'mozarella', 'without', 'thin', 'crust'], ['i', 'like', 'one', 'pizza', 'with', 'anchovy', 'caramelize', 'red', 'onion', 'and', 'roast', 'green', 'pepper', 'without', 'thin', 'crust'], ['i', 'like', 'one', 'pizza', 'with', 'applewood', 'bacon', 'grill', 'pineapple', 'and', 'shrimp', 'without', 'thin', 'crust'], ['i', 'like', 'one', 'pizza', 'with', 'pesto', 'sauce', 'roast', 'pepper', 'and', 'peppperonis', 'without', 'thin', 'crust'], ['i', 'like', 'one', 'pizza', 'with', 'peperronni', 'spicy', 'red', 'sauce', 'and', 'mushroom', 'without', 'thin', 'crust']]\n",
      "[[0, 0, 1, 0, 0, 4, 4, 4, 4, 0, 4, 0, 7, 7], [0, 0, 1, 0, 0, 4, 4, 4, 4, 0, 4, 4, 4, 0, 7, 7], [0, 0, 1, 0, 0, 4, 4, 4, 4, 0, 4, 0, 7, 7], [0, 0, 1, 0, 0, 4, 4, 4, 4, 0, 4, 0, 7, 7], [0, 0, 1, 0, 0, 4, 4, 4, 4, 0, 4, 0, 7, 7]]\n"
     ]
    }
   ],
   "source": [
    "# data, labels = load_data_labels('training_data_processed.txt', 'train_order_category_labels.txt')\n",
    "data, labels = load_data_labels('train_data_order_details.txt', 'train_labels_order_details.txt')\n",
    "# data2, labels2 = load_data_labels('synthetic_orders.txt', 'synthetic_labels.txt')\n",
    "# data.extend(data2)\n",
    "# labels.extend(labels2)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "print(data[:5])\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_data, dev_labels = load_data_labels('dev_data_processed.txt', 'dev_order_category_labels.txt')\n",
    "dev_data, dev_labels = load_data_labels('dev_data_order_details.txt', 'dev_labels_order_details.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = gs.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'like', 'one', 'pizza', 'with', 'red', 'onion', 'fry', 'onion', 'unk', 'mozarella', 'without', 'thin', 'crust'], ['i', 'like', 'one', 'pizza', 'with', 'anchovy', 'caramelize', 'red', 'onion', 'unk', 'roast', 'green', 'pepper', 'without', 'thin', 'crust'], ['i', 'like', 'one', 'pizza', 'with', 'applewood', 'bacon', 'grill', 'pineapple', 'unk', 'shrimp', 'without', 'thin', 'crust'], ['i', 'like', 'one', 'pizza', 'with', 'pesto', 'sauce', 'roast', 'pepper', 'unk', 'unk', 'without', 'thin', 'crust'], ['i', 'like', 'one', 'pizza', 'with', 'unk', 'spicy', 'red', 'sauce', 'unk', 'mushroom', 'without', 'thin', 'crust']]\n"
     ]
    }
   ],
   "source": [
    "#! get v Aand replace unknown words with unk token\n",
    "def process_sentence(sentence, model):\n",
    "    for i, word in enumerate(sentence):\n",
    "        if word not in model:\n",
    "            sentence[i] = 'unk'\n",
    "    return sentence\n",
    "\n",
    "data = [process_sentence(sentence, pretrained_model) for sentence in data]\n",
    "print(data[:5])\n",
    "vocab=set()\n",
    "for sentence in data:\n",
    "    vocab.update(sentence)\n",
    "#! get word index for each word in vocab\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=300\n",
    "input_dim=len(vocab)\n",
    "output_dim=11\n",
    "max_length=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokens in dev_data:\n",
    "    for i,word in enumerate(tokens):\n",
    "        if word not in vocab:\n",
    "            tokens[i] = 'unk'\n",
    "X_d=[[word2idx[word] for word in sentence] for sentence in dev_data]\n",
    "X_d=pad_sequences(X_d, maxlen=max_length, padding='post', value=-1)\n",
    "Y_d=pad_sequences(dev_labels, maxlen=max_length, padding='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! get embeddings matrix\n",
    "def get_embeddings_matrix(model, vocab):\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_dim = model.vector_size\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for i, word in enumerate(vocab):\n",
    "        if word in model:\n",
    "            embedding_matrix[i] = model[word]\n",
    "    return embedding_matrix\n",
    "embedding_matrix = get_embeddings_matrix(pretrained_model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! replace words with their index in vocab and pad sentences\n",
    "X=[[word2idx[word] for word in sentence] for sentence in data]\n",
    "X=pad_sequences(X, maxlen=max_length, padding='post', value=-1)\n",
    "Y=pad_sequences(labels, maxlen=max_length, padding='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_seq = Input(shape=(None,), dtype=tf.int32)\n",
    "# masked_input = Masking(mask_value=-1)(input_seq)\n",
    "\n",
    "# embedding = Embedding(input_dim=input_dim, output_dim=embedding_dim, weights=[embedding_matrix], trainable=True, mask_zero=False)(masked_input)\n",
    "\n",
    "# lstm_output = Bidirectional(LSTM(32, return_sequences=True))(embedding)\n",
    "\n",
    "# query = LSTM(32, return_sequences=True)(lstm_output)  \n",
    "# key = LSTM(32, return_sequences=True)(lstm_output)    \n",
    "# value = LSTM(32, return_sequences=True)(lstm_output)  \n",
    "\n",
    "# attention_output = Attention(use_scale=True, causal=False)([query, key, value])\n",
    "\n",
    "# dropout_output = Dropout(0.8)(attention_output)\n",
    "# output = Dense(output_dim, activation='softmax')(dropout_output)\n",
    "\n",
    "# model = Model(inputs=input_seq, outputs=output)\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer=optimizer,\n",
    "#     loss='sparse_categorical_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "#     monitor='val_loss',\n",
    "#     factor=0.5,\n",
    "#     patience=5,\n",
    "#     min_lr=1e-9,\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(\n",
    "#     X, \n",
    "#     Y, \n",
    "#     validation_data=(X_d, Y_d),  # Add validation data\n",
    "#     callbacks=[lr_scheduler], \n",
    "#     epochs=25,\n",
    "#     batch_size=1024\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Masking(mask_value=-1),  #! Masking layer to handle the padded values\n",
    "    tf.keras.layers.Embedding(input_dim=input_dim, output_dim=embedding_dim, weights=[embedding_matrix], trainable=True,mask_zero=False),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.Dropout(0.6),\n",
    "    tf.keras.layers.Dense(output_dim, activation='softmax')\n",
    "])\n",
    "## dropout->.8 , layers->48 best acc->70%\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',           #! Monitor the validation loss (you can use 'accuracy' or another metric)\n",
    "    factor=0.5,               #! Factor by which the learning rate will be reduced\n",
    "    patience=5,               #! Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=1e-9,              #! Lower bound on the learning rate\n",
    "    verbose=1                 #! Print a message when the learning rate is reduced\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "100/100 [==============================] - 9s 51ms/step - loss: 0.4872 - accuracy: 0.9373 - val_loss: 0.0559 - val_accuracy: 0.9879 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 0.0495 - accuracy: 0.9877 - val_loss: 0.0196 - val_accuracy: 0.9970 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.0199 - accuracy: 0.9956 - val_loss: 0.0115 - val_accuracy: 0.9981 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 0.0114 - accuracy: 0.9976 - val_loss: 0.0087 - val_accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.0076 - accuracy: 0.9984 - val_loss: 0.0072 - val_accuracy: 0.9986 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 0.0055 - accuracy: 0.9989 - val_loss: 0.0062 - val_accuracy: 0.9987 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.0055 - val_accuracy: 0.9988 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0050 - val_accuracy: 0.9988 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0047 - val_accuracy: 0.9989 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.0044 - val_accuracy: 0.9989 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.0042 - val_accuracy: 0.9990 - lr: 0.0010\n",
      "Epoch 12/25\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0040 - val_accuracy: 0.9992 - lr: 0.0010\n",
      "Epoch 13/25\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0037 - val_accuracy: 0.9992 - lr: 0.0010\n",
      "Epoch 14/25\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0037 - val_accuracy: 0.9992 - lr: 0.0010\n",
      "Epoch 15/25\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0036 - val_accuracy: 0.9992 - lr: 0.0010\n",
      "Epoch 16/25\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0035 - val_accuracy: 0.9992 - lr: 0.0010\n",
      "Epoch 17/25\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 9.8408e-04 - accuracy: 0.9998 - val_loss: 0.0033 - val_accuracy: 0.9993 - lr: 0.0010\n",
      "Epoch 18/25\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 8.9777e-04 - accuracy: 0.9999 - val_loss: 0.0032 - val_accuracy: 0.9993 - lr: 0.0010\n",
      "Epoch 19/25\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 8.0898e-04 - accuracy: 0.9999 - val_loss: 0.0030 - val_accuracy: 0.9993 - lr: 0.0010\n",
      "Epoch 20/25\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 7.5501e-04 - accuracy: 0.9999 - val_loss: 0.0030 - val_accuracy: 0.9993 - lr: 0.0010\n",
      "Epoch 21/25\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 7.0586e-04 - accuracy: 0.9999 - val_loss: 0.0029 - val_accuracy: 0.9994 - lr: 0.0010\n",
      "Epoch 22/25\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 6.5316e-04 - accuracy: 0.9999 - val_loss: 0.0028 - val_accuracy: 0.9994 - lr: 0.0010\n",
      "Epoch 23/25\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 5.9141e-04 - accuracy: 0.9999 - val_loss: 0.0027 - val_accuracy: 0.9994 - lr: 0.0010\n",
      "Epoch 24/25\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 5.6288e-04 - accuracy: 0.9999 - val_loss: 0.0026 - val_accuracy: 0.9994 - lr: 0.0010\n",
      "Epoch 25/25\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 5.1798e-04 - accuracy: 0.9999 - val_loss: 0.0026 - val_accuracy: 0.9994 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a08aff58e0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X was index of token in embedding matrix\n",
    "# X -> \n",
    "\n",
    "model.fit(\n",
    "    X, \n",
    "    Y, \n",
    "    validation_data=(X_d, Y_d),  # Add validation data\n",
    "    callbacks=[lr_scheduler], \n",
    "    epochs=25,\n",
    "    batch_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking_4 (Masking)         (None, 100)               0         \n",
      "                                                                 \n",
      " embedding_4 (Embedding)     (None, 100, 300)          112800    \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 100, 128)         186880    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 100, 128)          0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100, 11)           1419      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 301,099\n",
      "Trainable params: 301,099\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1594/1594 [==============================] - 20s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "preds_train = model.predict(X)\n",
    "preds_train = np.argmax(preds_train, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.9972745098039215\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(X)):\n",
    "    mask=X[i]!=-1\n",
    "    if np.all(preds_train[i][mask]==Y[i][mask]):\n",
    "        count+=1\n",
    "print(f\"Accuracy on training data: {count/len(X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict on the development set\n",
    "# preds_dev = Bidirectional_LSTM_model.predict(X)\n",
    "# preds_dev = np.argmax(preds_dev, axis=-1)\n",
    "\n",
    "# count = 0  # Count of completely correct sequences\n",
    "# last_index_error_count = 0  # Count of sequences with only last index error\n",
    "\n",
    "# for i in range(len(data)):\n",
    "#     # Get original sequence lengths before padding\n",
    "#     original_length = len(data[i])\n",
    "    \n",
    "#     # Extract predictions and true labels for the original sequence\n",
    "#     pred_seq = preds_dev[i][:original_length]\n",
    "#     true_seq = labels[i]  # Original labels are unpadded\n",
    "    \n",
    "#     # Check if the sequence is entirely correct\n",
    "#     if (pred_seq == true_seq).all():\n",
    "#         count += 1\n",
    "#     else:\n",
    "#         # Check if only the last index is incorrect\n",
    "#         if (pred_seq[:-1] == true_seq[:-1]).all() and (pred_seq[-1] != true_seq[-1]):\n",
    "#             last_index_error_count += 1\n",
    "        \n",
    "#         # Print debug information for mismatches\n",
    "#         print(f\"Index with mismatch: {i}\")\n",
    "#         print(f\"Predicted: {pred_seq}\")\n",
    "#         print(f\"True:      {true_seq}\")\n",
    "#         print(\"--------------------------------------------\")\n",
    "\n",
    "# # Print results\n",
    "# print(f\"Sequences with only last index error: {last_index_error_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_data, dev_labels = load_data_labels('dev_data_processed.txt', 'dev_order_category_labels.txt')\n",
    "# for tokens in dev_data:\n",
    "#     for i,word in enumerate(tokens):\n",
    "#         if word not in vocab!= 'a':\n",
    "#             tokens[i] = 'unk'\n",
    "# X_d=[[word2idx[word] for word in sentence] for sentence in dev_data]\n",
    "# X_d=pad_sequences(X_d, maxlen=max_length, padding='post', value=-1)\n",
    "# Y_d=pad_sequences(dev_labels, maxlen=max_length, padding='post', value=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 16ms/step\n",
      "Index with mismatch: 74\n",
      "Predicted: [0 0 0 0 1 2 3 3 3 0 0 1 2 3 3 4 0]\n",
      "True:      [0, 0, 0, 0, 1, 2, 3, 3, 4, 0, 0, 1, 2, 3, 3, 4, 0]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 177\n",
      "Predicted: [0 0 0 0 1 2 0 0 4 4 0 4 0 0 0 1 9 0 8 0 0]\n",
      "True:      [0, 1, 0, 0, 0, 2, 0, 0, 4, 4, 0, 4, 0, 0, 0, 1, 9, 0, 8, 0, 0]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 188\n",
      "Predicted: [0 0 0 0 1 2 4 0 1 2 4 0 0 4 4 0 1 2 0 0 4 4 4 0 4 4]\n",
      "True:      [0, 1, 0, 0, 0, 2, 4, 0, 1, 2, 4, 0, 0, 4, 4, 0, 1, 2, 0, 0, 4, 4, 4, 0, 4, 4]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 192\n",
      "Predicted: [0 0 0 0 0 1 2 0 0 4 0 1 3 3 0 0 0 0 0 0 6 0 0 0]\n",
      "True:      [0, 0, 0, 0, 0, 1, 2, 0, 0, 4, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 193\n",
      "Predicted: [0 0 0 0 1 2 2 3 0 0 1 2 8]\n",
      "True:      [0, 1, 0, 0, 0, 2, 2, 3, 0, 0, 1, 2, 8]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 198\n",
      "Predicted: [0 0 1 0 0 1 2 3 0 0 1 8]\n",
      "True:      [0, 0, 0, 0, 0, 1, 2, 3, 0, 0, 1, 8]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 200\n",
      "Predicted: [0 0 0 1 0 0 1 2 0 0 4 4 0 4 4]\n",
      "True:      [0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 4, 4, 0, 4, 4]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 218\n",
      "Predicted: [0 0 1 2 1 3 0 0 4 0 4]\n",
      "True:      [0, 0, 1, 2, 0, 0, 0, 0, 4, 0, 4]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 228\n",
      "Predicted: [0 0 0 1 3 3 0 0 5 4]\n",
      "True:      [0, 0, 0, 1, 0, 3, 0, 0, 5, 4]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 229\n",
      "Predicted: [0 0 0 0 1 0 0 0 0 0 3 3 0 4 0 0 0 1 0 0 0 0 0 6 0 0]\n",
      "True:      [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 3, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 230\n",
      "Predicted: [0 0 0 0 0 1 2 3 3 0 0 4 4 0 0 4 0 1 4 0 4 4 0 1 2 8]\n",
      "True:      [0, 0, 0, 0, 0, 1, 2, 3, 3, 0, 0, 4, 4, 0, 0, 4, 0, 5, 5, 5, 4, 4, 0, 1, 2, 8]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 237\n",
      "Predicted: [0 1 0 1 2 0 0 4 4 0 4]\n",
      "True:      [0, 0, 0, 1, 2, 0, 0, 4, 4, 0, 4]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 240\n",
      "Predicted: [0 0 1 0 0 1 2 3 3 0 0 0 1 2 8 8]\n",
      "True:      [0, 0, 0, 0, 0, 1, 2, 3, 3, 0, 0, 0, 1, 2, 8, 8]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 242\n",
      "Predicted: [0 0 0 0 1 0 0 0 0 0 0 2 0 0 0 3 3 0 0 0 0 4 0 4 0]\n",
      "True:      [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 4, 0]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 243\n",
      "Predicted: [0 0 0 0 0 0 0 0 1 2 0 0 4 4 0 4]\n",
      "True:      [0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 4, 4, 0, 4]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 246\n",
      "Predicted: [0 0 1 2 2 0 0 4 0 4 0 0 5 4]\n",
      "True:      [0, 0, 1, 2, 0, 0, 0, 4, 0, 4, 0, 0, 5, 4]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 258\n",
      "Predicted: [0 0 0 0 0 0 0 1 0 0 0 0 2 0 0 0 0 4 4 0 0 0 5 4 0 0 4]\n",
      "True:      [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 4, 4, 0, 0, 0, 5, 4, 0, 0, 0]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 259\n",
      "Predicted: [0 0 0 1 2 3 3 0 0 0 4]\n",
      "True:      [0, 0, 0, 1, 2, 3, 3, 0, 0, 4, 4]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 260\n",
      "Predicted: [0 1 0 0 0 1 2 0 0 4 0 4 0 1 2 0 0 4 4]\n",
      "True:      [0, 0, 0, 0, 0, 1, 2, 0, 0, 4, 0, 4, 0, 1, 2, 0, 0, 4, 4]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 262\n",
      "Predicted: [0 0 0 1 2 0 0 1 4]\n",
      "True:      [0, 0, 0, 1, 2, 0, 0, 0, 4]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 271\n",
      "Predicted: [0 0 0 0 0 0 0 1 2 4 0 0 5 4 0 1 2 0 0 4]\n",
      "True:      [0, 0, 0, 0, 1, 0, 0, 0, 2, 4, 0, 0, 5, 4, 0, 1, 2, 0, 0, 4]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 274\n",
      "Predicted: [0 0 0 0 0 0 0 1 2 0 0 4 4 0 4]\n",
      "True:      [0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 4, 4, 0, 4]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 294\n",
      "Predicted: [0 0 0 0 0 0 0 1 0 0 0 0 1 2 0 0 4 0 4 0 0 0 0 0 7 7 0]\n",
      "True:      [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 4, 0, 4, 0, 0, 0, 0, 0, 7, 7, 0]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 309\n",
      "Predicted: [0 0 0 1 2 0 0 4 4 0 4]\n",
      "True:      [0, 0, 0, 1, 2, 0, 0, 0, 4, 0, 4]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 310\n",
      "Predicted: [0 0 1 2 2 0 0 4 0 4 0 0 0 6]\n",
      "True:      [0, 0, 1, 2, 0, 0, 0, 4, 0, 4, 0, 0, 0, 6]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 315\n",
      "Predicted: [0 0 0 0 0 0 0 0 1 2 0 0 4 0 4 4]\n",
      "True:      [0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 4, 0, 4, 4]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 317\n",
      "Predicted: [0 0 0 0 1 2 0 0 4 0 5 4 0 0 1 2 0 0 4 0 4]\n",
      "True:      [0, 1, 0, 0, 0, 2, 0, 0, 4, 0, 5, 4, 0, 0, 1, 2, 0, 0, 4, 0, 4]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 321\n",
      "Predicted: [0 0 1 4 0 4 0 0 1 7 7]\n",
      "True:      [0, 0, 1, 4, 0, 4, 0, 0, 0, 0, 0]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 329\n",
      "Predicted: [ 0  0  1  8  1 10  8  8  0  1  8  0  0]\n",
      "True:      [0, 0, 1, 8, 1, 2, 8, 8, 0, 1, 8, 0, 0]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 346\n",
      "Predicted: [0 1 0 0 0 1 2 4 0 0 4 0 0 0 1 9 0 8]\n",
      "True:      [0, 0, 0, 0, 0, 1, 2, 4, 0, 0, 4, 0, 0, 0, 1, 9, 0, 8]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 550\n",
      "Predicted: [0 0 1 2 0 1 8 0 1 2 8 8 0]\n",
      "True:      [0, 0, 1, 2, 8, 1, 8, 0, 1, 2, 8, 8, 0]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 571\n",
      "Predicted: [0 0 1 2 0 1 8 0 1 8 0]\n",
      "True:      [0, 0, 1, 2, 8, 1, 8, 0, 1, 8, 0]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 609\n",
      "Predicted: [0 0 0 0 0 0 0 1 2 3 3 0 1 2 8 0 1 2 8]\n",
      "True:      [0, 0, 0, 0, 1, 0, 0, 0, 2, 3, 3, 0, 1, 2, 8, 0, 1, 2, 8]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 622\n",
      "Predicted: [1 2 3 0 0 1 2 4 0]\n",
      "True:      [1, 2, 4, 0, 0, 1, 2, 4, 0]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 642\n",
      "Predicted: [0 0 1 0 1 8 0 1 2 8 8]\n",
      "True:      [0, 0, 1, 8, 1, 8, 0, 1, 2, 8, 8]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 709\n",
      "Predicted: [ 0  0  0  1 10 10 10  8  8  1  2  4  0  0  1  2  3  3  0]\n",
      "True:      [0, 0, 0, 1, 10, 10, 0, 8, 8, 1, 2, 4, 0, 0, 1, 2, 3, 3, 0]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 721\n",
      "Predicted: [0 0 0 1 2 0 0 4 0 4 0 0 0 0 0 4]\n",
      "True:      [0, 0, 0, 1, 2, 0, 0, 4, 0, 4, 0, 0, 0, 0, 0, 6]\n",
      "--------------------------------------------\n",
      "Index with mismatch: 781\n",
      "Predicted: [0 0 0 0 1 2 0 0 4 0 4 0 0 0 0 0 0 0 0 4]\n",
      "True:      [0, 0, 0, 0, 1, 2, 0, 0, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 6]\n",
      "--------------------------------------------\n",
      "Accuracy on dev_data: 0.9551\n",
      "Sequences with only last index error: 3\n"
     ]
    }
   ],
   "source": [
    "# Predict on the development set\n",
    "preds_dev = model.predict(X_d)\n",
    "preds_dev = np.argmax(preds_dev, axis=-1)\n",
    "\n",
    "count = 0  # Count of completely correct sequences\n",
    "last_index_error_count = 0  # Count of sequences with only last index error\n",
    "\n",
    "for i in range(len(dev_data)):\n",
    "    # Get original sequence lengths before padding\n",
    "    original_length = len(dev_data[i])\n",
    "    \n",
    "    # Extract predictions and true dev_labels for the original sequence\n",
    "    pred_seq = preds_dev[i][:original_length]\n",
    "    true_seq = dev_labels[i]  # Original dev_labels are unpadded\n",
    "    \n",
    "    # Check if the sequence is entirely correct\n",
    "    if (pred_seq == true_seq).all():\n",
    "        count += 1\n",
    "    else:\n",
    "        # Check if only the last index is incorrect\n",
    "        if (pred_seq[:-1] == true_seq[:-1]).all() and (pred_seq[-1] != true_seq[-1]):\n",
    "            last_index_error_count += 1\n",
    "        \n",
    "        # Print debug information for mismatches\n",
    "        print(f\"Index with mismatch: {i}\")\n",
    "        \n",
    "        print(f\"Predicted: {pred_seq}\")\n",
    "        print(f\"True:      {true_seq}\")\n",
    "        print(\"--------------------------------------------\")\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy on dev_data: {count / len(dev_data):.4f}\")\n",
    "print(f\"Sequences with only last index error: {last_index_error_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
