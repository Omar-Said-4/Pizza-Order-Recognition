{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\omara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import inflect\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "import ast\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('want', 'NN'), ('learning', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('with', 'IN'), ('Python', 'NNP'), ('fdsaf', 'NN')]\n",
      "[('want', 12), ('learning', 28), ('natural', 7), ('language', 12), ('processing', 12), ('with', 6), ('Python', 14), ('fdsaf', 12)]\n",
      "[12, 28, 7, 12, 12, 6, 14, 12]\n"
     ]
    }
   ],
   "source": [
    "# Mapping POS tags to numbers\n",
    "pos_to_number = {\n",
    "    'CC': 1, 'CD': 2, 'DT': 3, 'EX': 4, 'FW': 5, 'IN': 6, 'JJ': 7, \n",
    "    'JJR': 8, 'JJS': 9, 'LS': 10, 'MD': 11, 'NN': 12, 'NNS': 13,\n",
    "    'NNP': 14, 'NNPS': 15, 'PDT': 16, 'POS': 17, 'PRP': 18, 'PRP$': 19, \n",
    "    'RB': 20, 'RBR': 21, 'RBS': 22, 'RP': 23, 'TO': 24, 'UH': 25,\n",
    "    'VB': 26, 'VBD': 27, 'VBG': 28, 'VBN': 29, 'VBP': 30, 'VBZ': 31,\n",
    "    'WDT': 32, 'WP': 33, 'WP$': 34, 'WRB': 35\n",
    "}\n",
    "\n",
    "# Example of tagging a sentence\n",
    "import nltk\n",
    "\n",
    "sentence = \" want learning natural language processing with Python fdsaf\"\n",
    "words = nltk.word_tokenize(sentence)\n",
    "tags = nltk.pos_tag(words)\n",
    "\n",
    "# Map POS tags to numbers\n",
    "tagged_with_numbers = [(word, pos_to_number.get(tag, -1)) for word, tag in tags]\n",
    "numbers_only = [pos_to_number.get(tag, -1) for _, tag in tags]\n",
    "\n",
    "print(tags)\n",
    "print(tagged_with_numbers)\n",
    "print(numbers_only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_labels(data_path, labels_path):\n",
    "    with open(data_path, 'r') as f:\n",
    "        data = [ast.literal_eval(line.strip()) for line in f]\n",
    "    with open(labels_path, 'r') as f:\n",
    "        labels = [ast.literal_eval(line.strip()) for line in f]\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['can', 'i', 'have', 'one', 'large', 'bbq', 'pull', 'pork'], ['large', 'pie', 'with', 'green', 'pepper', 'and', 'with', 'extra', 'peperonni'], ['i', 'like', 'one', 'large', 'vegetarian', 'pizza'], ['party', 'size', 'stuff', 'crust', 'pie', 'with', 'american', 'cheese', 'and', 'with', 'mushroom'], ['can', 'i', 'have', 'one', 'personal', 'size', 'artichoke']]\n",
      "[[2, 2, 2, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "data, labels = load_data_labels('training_data_processed.txt','train_order_category_labels.txt')\n",
    "print(data[:5])\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'want', 'to', 'order', 'two', 'medium', 'pizza', 'with', 'sausage', 'and', 'black', 'olive', 'and', 'two', 'medium', 'pizza', 'with', 'pepperoni', 'and', 'extra', 'cheese', 'and', 'three', 'large', 'pizza', 'with', 'pepperoni', 'and', 'sausage'], ['five', 'medium', 'pizza', 'with', 'tomato', 'and', 'ham'], ['i', 'need', 'to', 'order', 'one', 'large', 'vegetarian', 'pizza', 'with', 'extra', 'banana', 'pepper'], ['i', 'like', 'to', 'order', 'one', 'large', 'onion', 'and', 'pepper', 'pizza'], ['i', 'll', 'have', 'one', 'pie', 'along', 'with', 'pesto', 'and', 'ham', 'but', 'avoid', 'olive']]\n",
      "[[2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 0, 0, 0, 0, 0, 0], [2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "dev_data, dev_labels = load_data_labels('dev_data_processed.txt', 'dev_order_category_labels.txt')\n",
    "print(dev_data[:5])\n",
    "print(dev_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_labels=[]\n",
    "for order in data:\n",
    "    order = [word.upper() if word == 'i' else word for word in order]\n",
    "    tags= nltk.pos_tag(order)\n",
    "    tags_numbers = [pos_to_number.get(tag, -1) for _, tag in tags]\n",
    "    train_tag_labels.append(tags_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_tag_labels=[]\n",
    "for order in dev_data:\n",
    "    order = [word.upper() if word == 'i' else word for word in order]\n",
    "    tags= nltk.pos_tag(order)\n",
    "    tags_numbers = [pos_to_number.get(tag, -1) for _, tag in tags]\n",
    "    dev_tag_labels.append(tags_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_tag_labels.txt', 'w') as f:\n",
    "    for item in train_tag_labels:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open('dev_tag_labels.txt', 'w') as f:\n",
    "    for item in dev_tag_labels:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
