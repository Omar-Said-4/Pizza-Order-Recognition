{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clean-text in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clean-text) (1.7.0)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clean-text) (6.3.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: inflect in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (7.4.0)\n",
      "Requirement already satisfied: more-itertools>=8.5.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from inflect) (10.5.0)\n",
      "Requirement already satisfied: typeguard>=4.0.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from inflect) (4.4.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typeguard>=4.0.1->inflect) (4.12.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install clean-text\n",
    "!pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import inflect\n",
    "from cleantext import clean\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import ast\n",
    "import copy\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = inflect.engine()\n",
    "\n",
    "def replace_numbers_with_words(text):\n",
    "    number_pattern = r'\\b\\d+\\b'\n",
    "    \n",
    "    def number_to_words(match):\n",
    "        number = int(match.group())\n",
    "        # Convert to words and replace hyphens with underscores\n",
    "        return p.number_to_words(number, andword=\"\").replace('-', '_')\n",
    "\n",
    "    \n",
    "    text = re.sub(number_pattern, number_to_words, text)\n",
    "    \n",
    "    text = re.sub(\n",
    "        r'\\b(a|an)\\b(?! ((lot)|(little)|(few)|(number)|(couple)|(bit)|(load)|(stack)|(bunch)|(group)|(set)|(series)|(variety)|(range)|(amount)|(sum)|(total)))',\n",
    "       'one',\n",
    "        text,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def replace_numbers_with_words2(text):\n",
    "    number_pattern = r'\\b\\d+\\b'\n",
    "    \n",
    "    def number_to_words(match):\n",
    "        number = int(match.group())\n",
    "        # Convert to words and replace hyphens with underscores\n",
    "        return p.number_to_words(number, andword=\"\").replace('-', '_')\n",
    "\n",
    "    \n",
    "    text = re.sub(number_pattern, number_to_words, text)\n",
    "    return text\n",
    "\n",
    "def handle_negations(text):\n",
    "    negations_pattern = r\"\\b(?:no|not|without)\\s+.*?\\b(?=(?:[^\\w\\s]|$))\"\n",
    "    text = re.sub(negations_pattern, lambda x: ' '.join([f'not_{word}' for word in x.group(0).split()]), text)\n",
    "    return text\n",
    "\n",
    "def lem_word(word):\n",
    "    if(word=='bit'):\n",
    "        return word\n",
    "    possible_pos = [wordnet.NOUN, wordnet.VERB, wordnet.ADJ, wordnet.ADV]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for pos in possible_pos:\n",
    "        word=lemmatizer.lemmatize(word,pos)\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = set(stopwords.words('english'))\n",
    "stopwords.add('like')\n",
    "not_stopwords = ['not_' + word for word in stopwords]\n",
    "stopwords=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(line.strip())\n",
    "    return data\n",
    "test_data=load_data('test_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(vocab_file):\n",
    "    with open(vocab_file, 'r') as f:\n",
    "        vocab = set([line.strip() for line in f.readlines()])\n",
    "    return vocab\n",
    "\n",
    "# Load word2idx from JSON file\n",
    "def load_word2idx(word2idx_file):\n",
    "    with open(word2idx_file, 'r') as f:\n",
    "        word2idx = json.load(f)\n",
    "    return word2idx\n",
    "vocab=load_vocab('vocab.txt')\n",
    "word2idx=load_word2idx('word2idx.json')\n",
    "word2idx['UNK']=len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'will', 'go', 'with', 'one', 'medium', 'pizza', 'with', 'pepproni', 'and', 'sausage']]\n"
     ]
    }
   ],
   "source": [
    "#! tokenize test data and remove stop words\n",
    "def preprocess_test_data(test_data, stopwords,with_a=True):\n",
    "    \n",
    "    test_data=[replace_numbers_with_words(order) for order in test_data] if with_a else [replace_numbers_with_words2(order) for order in test_data]\n",
    "    test_data = [word_tokenize(order) for order in test_data]\n",
    "    test_data = [[word.lower() for word in order if word.lower() not in stopwords] for order in test_data]\n",
    "    test_data = [clean(order, no_line_breaks=True, no_punct=True, no_currency_symbols=True) for order in test_data]\n",
    "    test_data = [re.sub(r'\\bd\\s+', '', order) for order in test_data]\n",
    "    test_data = [word_tokenize(order) for order in test_data]\n",
    "    test_data = [[lem_word(word) for word in order] for order in test_data]\n",
    "    return test_data\n",
    "order_tokens_1 = preprocess_test_data(test_data, stopwords)\n",
    "order_tokens_2 = preprocess_test_data(test_data, stopwords,with_a=False)\n",
    "print(order_tokens_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model1\n",
    "order_category_labels = [\n",
    "    [2, 2, 2, 2, 0, 0, 0, 2, 0, 2, 0],  # model 1 output\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'will', 'go', 'with', 'one', 'medium', 'pizza', 'with', 'pepproni', 'and', 'sausage']]\n",
      "[[2, 2, 2, 2, 0, 0, 0, 2, 0, 2, 0]]\n"
     ]
    }
   ],
   "source": [
    "test_data,test_labels_order_category=order_tokens_1,order_category_labels\n",
    "print(test_data)\n",
    "print(test_labels_order_category)\n",
    "test_data_copy = copy.deepcopy(test_data)  # Deep copy of dev_data\n",
    "for tokens in test_data:\n",
    "    for i,word in enumerate(tokens):\n",
    "        if word not in vocab:\n",
    "            tokens[i] = 'UNK'\n",
    "X_test=[[word2idx[word] for word in sentence] for sentence in test_data]\n",
    "X_test=pad_sequences(X_test, maxlen=100, padding='post', value=-1)\n",
    "X_categories_test = [[category for category in sentence_categories] for sentence_categories in order_category_labels]\n",
    "X_categories_test = pad_sequences(X_categories_test, maxlen=100, padding='post', value=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=300\n",
    "input_dim=len(vocab)\n",
    "output_dim=11\n",
    "max_length=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "def create_category_mask(categories):\n",
    "    category_to_mask = tf.constant([\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],  # PIZZA\n",
    "        [1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1],  # DRINK\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   # NEITHER\n",
    "    ], dtype=tf.float32)\n",
    "\n",
    "    categories = tf.where(categories == -1, 2, categories)\n",
    "    mask = tf.gather(category_to_mask, categories)\n",
    "    return mask\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def create_category_mask_output_shape(input_shape):\n",
    "    return (input_shape[0], input_shape[1], output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.models.load_model('Order_details_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "[[0, 0, 0, 0, 6, 1, 3, 0, 3, 0, 6]]\n"
     ]
    }
   ],
   "source": [
    "pred_test = model2.predict([X_test, X_categories_test])\n",
    "pred_test = np.argmax(pred_test, axis=-1)  # Get the class with the highest probability\n",
    "pred_test = [seq[:len(test_data[i])] for i, seq in enumerate(pred_test)]\n",
    "pred_test = [list(seq) for seq in pred_test] \n",
    "print(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine_tokens(tokens, labels):\n",
    "    combined_tokens = []\n",
    "    combined_labels = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(tokens):\n",
    "        if labels[i] != 0 and labels[i]!=4:  # Check for non-None labels\n",
    "            combined_token = tokens[i]\n",
    "            current_label = labels[i]\n",
    "            i += 1\n",
    "\n",
    "            while i < len(tokens) and labels[i] == current_label:\n",
    "                combined_token += f\"_{tokens[i]}\"\n",
    "                i += 1\n",
    "\n",
    "            combined_tokens.append(combined_token)\n",
    "            combined_labels.append(current_label)\n",
    "        else:\n",
    "            combined_tokens.append(tokens[i])\n",
    "            combined_labels.append(labels[i])\n",
    "            i += 1\n",
    "\n",
    "    return combined_tokens, combined_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    0: \"NONE\",\n",
    "    1: \"NUMBER\",\n",
    "    2: \"SIZE\",\n",
    "    3: \"STYLE\",\n",
    "    4: \"TOPPING\",\n",
    "    5: \"QUANTITY\",\n",
    "    6: \"NOT_TOPPING\",\n",
    "    7: \"NOT_STYLE\",\n",
    "    8: \"DRINKTYPE\",\n",
    "    9: \"CONTAINERTYPE\",\n",
    "    10: \"VOLUME\",\n",
    "}\n",
    "\n",
    "def Get_EXR_Format(tokens, labels):\n",
    "    # Step 1: Split into suborders\n",
    "    suborders = []\n",
    "    current_suborder = {\n",
    "        \"tokens\": [],\n",
    "        \"labels\": [],\n",
    "        \"has_number\": False,\n",
    "        \"has_size\": False,\n",
    "        \"has_style\": False,\n",
    "        \"has_drinktype\": False,\n",
    "        \"has_volume\": False,\n",
    "        \"has_containertype\": False\n",
    "    }\n",
    "    \n",
    "    for token, label in zip(tokens, labels):\n",
    "        if label == 1 and current_suborder[\"tokens\"]:  # New suborder starts at 'Number'\n",
    "            suborders.append(current_suborder)\n",
    "            current_suborder = {\n",
    "                \"tokens\": [],\n",
    "                \"labels\": [],\n",
    "                \"has_number\": False,\n",
    "                \"has_size\": False,\n",
    "                \"has_style\": False,\n",
    "                \"has_drinktype\": False,\n",
    "                \"has_volume\": False,\n",
    "                \"has_containertype\": False\n",
    "            }\n",
    "        \n",
    "        if label == 2 and current_suborder[\"has_size\"] and current_suborder[\"tokens\"]:\n",
    "            suborders.append(current_suborder)\n",
    "            current_suborder = {\n",
    "                \"tokens\": [],\n",
    "                \"labels\": [],\n",
    "                \"has_number\": False,\n",
    "                \"has_size\": False,\n",
    "                \"has_style\": False,\n",
    "                \"has_drinktype\": False,\n",
    "                \"has_volume\": False,\n",
    "                \"has_containertype\": False\n",
    "            }\n",
    "        \n",
    "        if label == 3 and current_suborder[\"has_style\"] and current_suborder[\"tokens\"]:\n",
    "            suborders.append(current_suborder)\n",
    "            current_suborder = {\n",
    "                \"tokens\": [],\n",
    "                \"labels\": [],\n",
    "                \"has_number\": False,\n",
    "                \"has_size\": False,\n",
    "                \"has_style\": False,\n",
    "                \"has_drinktype\": False,\n",
    "                \"has_volume\": False,\n",
    "                \"has_containertype\": False\n",
    "            }\n",
    "        if label == 8 and current_suborder[\"has_drinktype\"] and current_suborder[\"tokens\"]:\n",
    "            suborders.append(current_suborder)\n",
    "            current_suborder = {\n",
    "                \"tokens\": [],\n",
    "                \"labels\": [],\n",
    "                \"has_number\": False,\n",
    "                \"has_size\": False,\n",
    "                \"has_style\": False,\n",
    "                \"has_drinktype\": False,\n",
    "                \"has_volume\": False,\n",
    "                \"has_containertype\": False\n",
    "            }\n",
    "        if label == 10 and current_suborder[\"has_volume\"] and current_suborder[\"tokens\"]:\n",
    "            suborders.append(current_suborder)\n",
    "            current_suborder = {\n",
    "                \"tokens\": [],\n",
    "                \"labels\": [],\n",
    "                \"has_number\": False,\n",
    "                \"has_size\": False,\n",
    "                \"has_style\": False,\n",
    "                \"has_drinktype\": False,\n",
    "                \"has_volume\": False,\n",
    "                \"has_containertype\": False\n",
    "            }\n",
    "        if label == 9 and current_suborder[\"has_containertype\"] and current_suborder[\"tokens\"]:\n",
    "            suborders.append(current_suborder)\n",
    "            current_suborder = {\n",
    "                \"tokens\": [],\n",
    "                \"labels\": [],\n",
    "                \"has_number\": False,\n",
    "                \"has_size\": False,\n",
    "                \"has_style\": False,\n",
    "                \"has_drinktype\": False,\n",
    "                \"has_volume\": False,\n",
    "                \"has_containertype\": False\n",
    "            }\n",
    "        \n",
    "        current_suborder[\"tokens\"].append(token)\n",
    "        current_suborder[\"labels\"].append(label)\n",
    "\n",
    "        if label == 2:  \n",
    "            current_suborder[\"has_size\"] = True\n",
    "        elif label == 3: \n",
    "            current_suborder[\"has_style\"] = True\n",
    "        elif label == 1:  \n",
    "            current_suborder[\"has_number\"] = True\n",
    "        elif label == 8:  \n",
    "            current_suborder[\"has_drinktype\"] = True\n",
    "        elif label == 10: \n",
    "            current_suborder[\"has_volume\"] = True\n",
    "        elif label == 9:  \n",
    "            current_suborder[\"has_containertype\"] = True\n",
    "    \n",
    "    # Append the last suborder\n",
    "    if current_suborder[\"tokens\"]:\n",
    "        suborders.append(current_suborder)\n",
    "    \n",
    "    for suborder in suborders:\n",
    "        if not suborder[\"has_number\"]:\n",
    "            suborder[\"tokens\"].insert(0, \"one\")\n",
    "            suborder[\"labels\"].insert(0, 1)\n",
    "    \n",
    "    # Step 3: Classify suborders\n",
    "    order_details = []\n",
    "    for suborder in suborders:\n",
    "        tokens = suborder[\"tokens\"]\n",
    "        labels = suborder[\"labels\"]\n",
    "        if any(label in [8, 9, 10] for label in labels):  # Drink order\n",
    "            order_type = \"DRINKORDER\"\n",
    "        elif any(label in [3, 4, 5, 6, 7] for label in labels):  # Pizza order\n",
    "            order_type = \"PIZZAORDER\"\n",
    "        else:\n",
    "            continue  # Skip invalid suborders\n",
    "        \n",
    "        suborder_details = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            token = tokens[i]\n",
    "            label = labels[i]\n",
    "            if label == 5:  # Quantity\n",
    "                if i != len(tokens) - 1:\n",
    "                    suborder_details.append(\n",
    "                        f\"(COMPLEX_TOPPING (QUANTITY {token.upper()}) (TOPPING {tokens[i + 1].upper()}))\"\n",
    "                    )\n",
    "                    i += 1\n",
    "                else:\n",
    "                    suborder_details.append(f\"({label_mapping[4]} {token.upper()})\")\n",
    "            elif label in [6, 7]:  # NOT cases (NOT_TOPPING, NOT_STYLE)\n",
    "                new_label = label_mapping[label][4:]  # Remove \"NOT_\" from the label string\n",
    "                suborder_details.append(\n",
    "                    f\"(NOT ({new_label} {token.upper()}))\"\n",
    "                )\n",
    "            elif label != 0:\n",
    "                suborder_details.append(f\"({label_mapping[label]} {token.upper()})\")\n",
    "            i += 1\n",
    "        \n",
    "        order_details.append(f\"({order_type} {' '.join(suborder_details)})\")\n",
    "    \n",
    "    return f\"(ORDER {' '.join(order_details)})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_to_dict(s):\n",
    "    def parse(tokens):\n",
    "        token = tokens.pop(0)\n",
    "        if token == '(':\n",
    "            key = tokens.pop(0)\n",
    "            nested = {}\n",
    "            while len(tokens) > 0 and tokens[0] != ')':\n",
    "                if tokens[0] == '(':\n",
    "                    temp_dict=parse(tokens)\n",
    "                    for k,v in temp_dict.items():\n",
    "                        if k in nested:\n",
    "                            nested[k].append(v)\n",
    "                        elif k == 'DRINKORDER' or k == 'PIZZAORDER' or k =='TOPPING' or k =='NOT' or k =='COMPLEX_TOPPING' or k == 'STYLE':\n",
    "                            nested[k]=[v]\n",
    "                        else:\n",
    "                            nested[k] = v\n",
    "                else:\n",
    "                    sub_key = tokens.pop(0)\n",
    "                    if tokens[0] == ')':\n",
    "                        tokens.pop(0)\n",
    "                        return {key: sub_key}\n",
    "            if len(tokens) > 0:\n",
    "                tokens.pop(0)  \n",
    "            return {key: nested}\n",
    "        elif token == ')':\n",
    "            return {}\n",
    "\n",
    "    # Tokenize the string\n",
    "    tokens = re.findall(r'\\(|\\)|\\w+', s)\n",
    "    return parse(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_toppings(order_json):\n",
    "\n",
    "    order = json.loads(order_json)\n",
    "    processed_order = {\"ORDER\": {\"PIZZAORDER\": [], \"DRINKORDER\": []}}\n",
    "    \n",
    "    for pizza in order[\"ORDER\"].get(\"PIZZAORDER\", []):\n",
    "        toppings = pizza.get(\"TOPPING\", [])\n",
    "        complex_toppings = pizza.get(\"COMPLEX_TOPPING\", [])\n",
    "        excluded = pizza.get(\"NOT\", {})\n",
    "        excluded_toppings = []\n",
    "        tops=[]\n",
    "        styles=[]\n",
    "        excluded_styles=[]\n",
    "        all_styles=[]\n",
    "        for exc in excluded:\n",
    "            tops.append(exc.get(\"TOPPING\", []))\n",
    "            if exc.get(\"STYLE\", False):\n",
    "                excluded_styles.append(exc.get(\"STYLE\", {})[0])\n",
    "        styles=pizza.get(\"STYLE\", [])         \n",
    "        for top in tops:\n",
    "            for i in range(len(top)):\n",
    "                excluded_toppings.append(top[i])\n",
    "            \n",
    "        if \"NOT\" in pizza:\n",
    "            del pizza[\"NOT\"]\n",
    "        if \"STYLE\" in pizza: \n",
    "            del pizza[\"STYLE\"]\n",
    "        if \"TOPPING\" in pizza:\n",
    "            del pizza[\"TOPPING\"]\n",
    "        if \"COMPLEX_TOPPING\" in pizza:\n",
    "            del pizza[\"COMPLEX_TOPPING\"]\n",
    "        all_toppings = []\n",
    "        \n",
    "        for topping in toppings:\n",
    "            all_toppings.append({\n",
    "                \"NOT\": False,\n",
    "                \"Quantity\": None,  \n",
    "                \"Topping\": topping\n",
    "            })\n",
    "        for style in styles:\n",
    "            all_styles.append({\n",
    "                \"NOT\": False,\n",
    "                \"TYPE\": style\n",
    "            })\n",
    "        for style in excluded_styles:\n",
    "            all_styles.append({\n",
    "                \"NOT\": True,\n",
    "                \"TYPE\": style\n",
    "            })\n",
    "        \n",
    "        for topping in excluded_toppings:\n",
    "            all_toppings.append({\n",
    "                \"NOT\": True,\n",
    "                \"Quantity\": None,  \n",
    "                \"Topping\": topping\n",
    "            })\n",
    "        for complex_topping in complex_toppings:\n",
    "            complex_quantity = complex_topping.get(\"QUANTITY\", None)\n",
    "            complex_topping_list = complex_topping.get(\"TOPPING\", [])\n",
    "            for topping in complex_topping_list:\n",
    "                all_toppings.append({\n",
    "                    \"NOT\": False,\n",
    "                    \"Quantity\": complex_quantity,\n",
    "                    \"Topping\": topping\n",
    "                })\n",
    "        for drink in order[\"ORDER\"].get(\"DRINKORDER\", []):\n",
    "            processed_order[\"ORDER\"][\"DRINKORDER\"].append(drink)\n",
    "        # Add the pizza order with updated AllTopping\n",
    "        new_pizza_order = {**pizza, \"AllTopping\": all_toppings, \"STYLE\": all_styles}\n",
    "        processed_order[\"ORDER\"][\"PIZZAORDER\"].append(new_pizza_order)\n",
    "    return json.dumps(processed_order,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_order(order_json):\n",
    "    # Parse the input JSON\n",
    "    order = json.loads(order_json)\n",
    "    processed_order = {\"ORDER\": {\"PIZZAORDER\": [], \"DRINKORDER\": []}}\n",
    "    \n",
    "    for pizza in order[\"ORDER\"].get(\"PIZZAORDER\", []):\n",
    "        number = pizza.get(\"NUMBER\", None)\n",
    "        size = pizza.get(\"SIZE\", None)\n",
    "        style = pizza.get(\"STYLE\", None) \n",
    "        type_ = pizza.get(\"TYPE\", None)  \n",
    "        toppings = pizza.get(\"AllTopping\", [])\n",
    "        \n",
    "        excluded_toppings = pizza.get(\"NOT\", {}).get(\"TOPPING\", [])\n",
    "        toppings = [topping for topping in toppings if topping not in excluded_toppings]\n",
    "        \n",
    "        processed_order[\"ORDER\"][\"PIZZAORDER\"].append({\n",
    "            \"NUMBER\": number,\n",
    "            \"SIZE\": size,\n",
    "            \"STYLE\": style,\n",
    "            \"ALLTOPPING\": toppings\n",
    "        })\n",
    "    for drink in order[\"ORDER\"].get(\"DRINKORDER\", []):\n",
    "        number = drink.get(\"NUMBER\", None)\n",
    "        size = drink.get(\"SIZE\", None)\n",
    "        drink_type = drink.get(\"DRINKTYPE\", None)\n",
    "        container_type = drink.get(\"CONTAINERTYPE\", None)\n",
    "\n",
    "        processed_order[\"ORDER\"][\"DRINKORDER\"].append({\n",
    "            \"NUMBER\": number,\n",
    "            \"SIZE\": size,\n",
    "            \"DRINKTYPE\": drink_type,\n",
    "            \"CONTAINERTYPE\": container_type\n",
    "        })\n",
    "    \n",
    "    return json.dumps(processed_order, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json(data):\n",
    "    if isinstance(data, dict):\n",
    "        # Process dictionary keys and values recursively\n",
    "        return {key: process_json(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        # Process each element in the list recursively\n",
    "        return [process_json(item) for item in data]\n",
    "    elif isinstance(data, str):\n",
    "        # Convert string to lowercase and handle underscores\n",
    "        return data.lower().replace('_', ' ')\n",
    "    else:\n",
    "        # Return data as-is if it's not a string, list, or dictionary\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0 {\n",
      "    \"ORDER\": {\n",
      "        \"PIZZAORDER\": [\n",
      "            {\n",
      "                \"NUMBER\": \"ONE\",\n",
      "                \"SIZE\": null,\n",
      "                \"STYLE\": [],\n",
      "                \"ALLTOPPING\": [\n",
      "                    {\n",
      "                        \"NOT\": true,\n",
      "                        \"Quantity\": null,\n",
      "                        \"Topping\": \"ONE\"\n",
      "                    }\n",
      "                ]\n",
      "            },\n",
      "            {\n",
      "                \"NUMBER\": \"MEDIUM\",\n",
      "                \"SIZE\": null,\n",
      "                \"STYLE\": [\n",
      "                    {\n",
      "                        \"NOT\": false,\n",
      "                        \"TYPE\": \"PIZZA\"\n",
      "                    }\n",
      "                ],\n",
      "                \"ALLTOPPING\": []\n",
      "            },\n",
      "            {\n",
      "                \"NUMBER\": \"ONE\",\n",
      "                \"SIZE\": null,\n",
      "                \"STYLE\": [\n",
      "                    {\n",
      "                        \"NOT\": false,\n",
      "                        \"TYPE\": \"PEPPRONI\"\n",
      "                    }\n",
      "                ],\n",
      "                \"ALLTOPPING\": [\n",
      "                    {\n",
      "                        \"NOT\": true,\n",
      "                        \"Quantity\": null,\n",
      "                        \"Topping\": \"SAUSAGE\"\n",
      "                    }\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"DRINKORDER\": []\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, sequence in enumerate(pred_test):\n",
    "    combined_tokens, combined_labels = combine_tokens(order_tokens_2[i], sequence)\n",
    "    EXR_Format = Get_EXR_Format(combined_tokens, combined_labels)\n",
    "    result = parse_to_dict(EXR_Format)\n",
    "    json_string = json.dumps(result)\n",
    "    processed_toppings = process_toppings(json_string)\n",
    "    Json_Format=  process_order(processed_toppings)\n",
    "    print(f\"test {i}\",Json_Format)\n",
    "    final_Json=process_json(json.loads(Json_Format))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
