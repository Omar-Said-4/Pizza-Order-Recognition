{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: clean-text in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clean-text) (1.7.0)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clean-text) (6.3.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: inflect in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (7.4.0)\n",
      "Requirement already satisfied: more-itertools>=8.5.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from inflect) (10.5.0)\n",
      "Requirement already satisfied: typeguard>=4.0.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from inflect) (4.4.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typeguard>=4.0.1->inflect) (4.12.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install clean-text\n",
    "!pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import inflect\n",
    "from cleantext import clean\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from Spelling_correction import find_closest_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(line.strip())  # Use strip() to remove leading/trailing whitespace\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "path='input.txt'\n",
    "test_data=load_data(path)\n",
    "print(len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an inflect engine\n",
    "p = inflect.engine()\n",
    "\n",
    "def replace_numbers_with_words(text):\n",
    "    # Regular expression to find numbers in the text\n",
    "    number_pattern = r'\\b\\d+\\b'\n",
    "    \n",
    "    # Function to convert a number match to its word representation\n",
    "    def number_to_words(match):\n",
    "        number = int(match.group())\n",
    "        # Convert to words and replace hyphens with underscores\n",
    "        return p.number_to_words(number, andword=\"\").replace('-', '_')\n",
    "\n",
    "    \n",
    "    # Substitute numbers with their word representation\n",
    "    text = re.sub(number_pattern, number_to_words, text)\n",
    "    \n",
    "    # Replace standalone 'a' or 'an' with 'one' where appropriate\n",
    "    text = re.sub(\n",
    "        r'\\b(a|an)\\b(?! ((lot)|(little)|(few)|(number)|(couple)|(bit)|(load)|(stack)|(bunch)|(group)|(set)|(series)|(variety)|(range)|(amount)|(sum)|(total)))',\n",
    "       'one',\n",
    "        text,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def replace_numbers_with_words2(text):\n",
    "    # Regular expression to find numbers in the text\n",
    "    number_pattern = r'\\b\\d+\\b'\n",
    "    \n",
    "    # Function to convert a number match to its word representation\n",
    "    def number_to_words(match):\n",
    "        number = int(match.group())\n",
    "        # Convert to words and replace hyphens with underscores\n",
    "        return p.number_to_words(number, andword=\"\").replace('-', '_')\n",
    "\n",
    "    \n",
    "    # Substitute numbers with their word representation\n",
    "    text = re.sub(number_pattern, number_to_words, text)\n",
    "\n",
    "def handle_negations(text):\n",
    "    #! negations are based on test set\n",
    "    negations_pattern = r\"\\b(?:no|not|without)\\s+.*?\\b(?=(?:[^\\w\\s]|$))\"\n",
    "    # print(re.findall(negations_pattern, text))\n",
    "    text = re.sub(negations_pattern, lambda x: ' '.join([f'not_{word}' for word in x.group(0).split()]), text)\n",
    "    return text\n",
    "\n",
    "#!# lemmatize words with all possible pos tags\n",
    "def lem_word(word):\n",
    "    if(word=='bit'):\n",
    "        return word\n",
    "    possible_pos = [wordnet.NOUN, wordnet.VERB, wordnet.ADJ, wordnet.ADV]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for pos in possible_pos:\n",
    "        word=lemmatizer.lemmatize(word,pos)\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = set(stopwords.words('english'))\n",
    "stopwords.add('like')\n",
    "not_stopwords = ['not_' + word for word in stopwords]\n",
    "stopwords=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = gs.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(sentence, model):\n",
    "    for i, word in enumerate(sentence):\n",
    "        if word not in model:\n",
    "            sentence[i] = 'unk'\n",
    "    return sentence\n",
    "\n",
    "data = [process_sentence(sentence, pretrained_model) for sentence in data]\n",
    "print(data[:5])\n",
    "vocab=set()\n",
    "for sentence in data:\n",
    "    vocab.update(sentence)\n",
    "#! get word index for each word in vocab\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'would', 'like', 'to', 'order', 'two', 'large', 'chicago', 'style', 'pizza', 'with', 'extra', 'cheese', 'pepperoni', 'no', 'onion', 'and', 'bacon'], ['and', 'three', 'small', 'thin', 'crust', 'pizza', 'with', 'a', 'lot', 'of', 'olive', 'and', 'ham', 'i', 'also', 'want', 'five', 'large', 'bottle', 'of', 'coke', 'and', 'two', 'medium', 'can', 'of', 'diet', 'pepsi']]\n"
     ]
    }
   ],
   "source": [
    "#! tokenize test data and remove stop words\n",
    "def preprocess_test_data(test_data, stopwords,with_a=True):\n",
    "    \n",
    "    test_data=[replace_numbers_with_words(order) for order in test_data] if with_a else [replace_numbers_with_words2(order) for order in test_data]\n",
    "    test_data = [word_tokenize(order) for order in test_data]\n",
    "    test_data = [[word.lower() for word in order if word.lower() not in stopwords] for order in test_data]\n",
    "    test_data = [clean(order, no_line_breaks=True, no_punct=True, no_currency_symbols=True) for order in test_data]\n",
    "    test_data = [re.sub(r'\\bd\\s+', '', order) for order in test_data]\n",
    "    test_data = [word_tokenize(order) for order in test_data]\n",
    "    test_data = [[lem_word(word) for word in order] for order in test_data]\n",
    "    return test_data\n",
    "order_tokens_1 = preprocess_test_data(test_data, stopwords)\n",
    "order_tokens_2 = preprocess_test_data(test_data, stopwords,with_a=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load  model1\n",
    "order_category_labels=[] # model 1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data,test_labels_order_category=order_tokens_1,order_category_labels\n",
    "print(test_data)\n",
    "print(test_labels_order_category)\n",
    "test_data_copy = copy.deepcopy(test_data)  # Deep copy of dev_data\n",
    "for tokens in test_data:\n",
    "    for i,word in enumerate(tokens):\n",
    "        if word not in vocab:\n",
    "            tokens[i] = 'unk'\n",
    "X_test=[[word2idx[word] for word in sentence] for sentence in test_data]\n",
    "X_test=pad_sequences(X_test, maxlen=max_length, padding='post', value=-1)\n",
    "X_categories_test = [[category for category in sentence_categories] for sentence_categories in test_labels_order_category]\n",
    "X_categories_test = pad_sequences(X_categories_test, maxlen=max_length, padding='post', value=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=load_model('model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model2.predict([X_test, X_categories_test])  # Pass both inputs as a list\n",
    "pred_test = np.argmax(pred_test, axis=-1)  # Get the class with the highest probability\n",
    "# print(pred_test[:5])\n",
    "pred_test = [seq[:len(test_data[i])] for i, seq in enumerate(pred_test)]  # Trim each sequence\n",
    "pred_test = [list(seq) for seq in pred_test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = ['can', 'you', 'please', 'order', 'one', 'large', 'pepperoni', 'pizza', 'and', 'one', 'medium', 'sausage', 'pizza', 'with', 'one', 'sprite', 'and', 'one', 'coke']\n",
    "# labels = [0,0,0,0,1,2,4,0,0,1,2,4,0,0,1,8,0,1,8]\n",
    "# tokens=['two', 'pizzes', 'one', 'with', 'pepproni', 'and','one','with','chicken']\n",
    "# labels=[1,0,1,0,4,0,1,0,4]\n",
    "# tokens=['want','one','pizza', 'with', 'pepproni', 'and','with','large','size','iced','tea']\n",
    "# labels=[0,1,0,0,4,0,0,2,2,8,8]\n",
    "def combine_tokens(tokens, labels):\n",
    "    combined_tokens = []\n",
    "    combined_labels = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(tokens):\n",
    "        if labels[i] != 0 and labels[i]!=4:  # Check for non-None labels\n",
    "            # Start combining consecutive tokens with the same label\n",
    "            combined_token = tokens[i]\n",
    "            current_label = labels[i]\n",
    "            i += 1\n",
    "\n",
    "            while i < len(tokens) and labels[i] == current_label:\n",
    "                combined_token += f\"_{tokens[i]}\"\n",
    "                i += 1\n",
    "\n",
    "            combined_tokens.append(combined_token)\n",
    "            combined_labels.append(current_label)\n",
    "        else:\n",
    "            # Keep tokens with label 0 as is\n",
    "            combined_tokens.append(tokens[i])\n",
    "            combined_labels.append(labels[i])\n",
    "            i += 1\n",
    "\n",
    "    return combined_tokens, combined_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    0: \"NONE\",\n",
    "    1: \"NUMBER\",\n",
    "    2: \"SIZE\",\n",
    "    3: \"STYLE\",\n",
    "    4: \"TOPPING\",\n",
    "    5: \"QUANTITY\",\n",
    "    6: \"NOT_TOPPING\",\n",
    "    7: \"NOT_STYLE\",\n",
    "    8: \"DRINKTYPE\",\n",
    "    9: \"CONTAINERTYPE\",\n",
    "    10: \"VOLUME\",\n",
    "}\n",
    "\n",
    "def Get_EXR_Format(tokens, labels):\n",
    "    # Step 1: Split into suborders\n",
    "    suborders = []\n",
    "    current_suborder = {\n",
    "        \"tokens\": [],\n",
    "        \"labels\": [],\n",
    "        \"has_number\": False,\n",
    "        \"has_size\": False,\n",
    "        \"has_style\": False,\n",
    "        \"has_drinktype\": False,\n",
    "        \"has_volume\": False,\n",
    "        \"has_containertype\": False\n",
    "    }\n",
    "    \n",
    "    for token, label in zip(tokens, labels):\n",
    "        if label == 1 and current_suborder[\"tokens\"]:  # New suborder starts at 'Number'\n",
    "            suborders.append(current_suborder)\n",
    "            current_suborder = {\n",
    "                \"tokens\": [],\n",
    "                \"labels\": [],\n",
    "                \"has_number\": False,\n",
    "                \"has_size\": False,\n",
    "                \"has_style\": False,\n",
    "                \"has_drinktype\": False,\n",
    "                \"has_volume\": False,\n",
    "                \"has_containertype\": False\n",
    "            }\n",
    "        \n",
    "        # If the label is SIZE and we already have a size, start a new suborder\n",
    "        if label == 2 and current_suborder[\"has_size\"] and current_suborder[\"tokens\"]:\n",
    "            suborders.append(current_suborder)\n",
    "            current_suborder = {\n",
    "                \"tokens\": [],\n",
    "                \"labels\": [],\n",
    "                \"has_number\": False,\n",
    "                \"has_size\": False,\n",
    "                \"has_style\": False,\n",
    "                \"has_drinktype\": False,\n",
    "                \"has_volume\": False,\n",
    "                \"has_containertype\": False\n",
    "            }\n",
    "        \n",
    "        # If the label is STYLE and we already have a style, start a new suborder\n",
    "        if label == 3 and current_suborder[\"has_style\"] and current_suborder[\"tokens\"]:\n",
    "            suborders.append(current_suborder)\n",
    "            current_suborder = {\n",
    "                \"tokens\": [],\n",
    "                \"labels\": [],\n",
    "                \"has_number\": False,\n",
    "                \"has_size\": False,\n",
    "                \"has_style\": False,\n",
    "                \"has_drinktype\": False,\n",
    "                \"has_volume\": False,\n",
    "                \"has_containertype\": False\n",
    "            }\n",
    "        if label == 8 and current_suborder[\"has_drinktype\"] and current_suborder[\"tokens\"]:\n",
    "            suborders.append(current_suborder)\n",
    "            current_suborder = {\n",
    "                \"tokens\": [],\n",
    "                \"labels\": [],\n",
    "                \"has_number\": False,\n",
    "                \"has_size\": False,\n",
    "                \"has_style\": False,\n",
    "                \"has_drinktype\": False,\n",
    "                \"has_volume\": False,\n",
    "                \"has_containertype\": False\n",
    "            }\n",
    "        if label == 10 and current_suborder[\"has_volume\"] and current_suborder[\"tokens\"]:\n",
    "            suborders.append(current_suborder)\n",
    "            current_suborder = {\n",
    "                \"tokens\": [],\n",
    "                \"labels\": [],\n",
    "                \"has_number\": False,\n",
    "                \"has_size\": False,\n",
    "                \"has_style\": False,\n",
    "                \"has_drinktype\": False,\n",
    "                \"has_volume\": False,\n",
    "                \"has_containertype\": False\n",
    "            }\n",
    "        if label == 9 and current_suborder[\"has_containertype\"] and current_suborder[\"tokens\"]:\n",
    "            suborders.append(current_suborder)\n",
    "            current_suborder = {\n",
    "                \"tokens\": [],\n",
    "                \"labels\": [],\n",
    "                \"has_number\": False,\n",
    "                \"has_size\": False,\n",
    "                \"has_style\": False,\n",
    "                \"has_drinktype\": False,\n",
    "                \"has_volume\": False,\n",
    "                \"has_containertype\": False\n",
    "            }\n",
    "        \n",
    "        current_suborder[\"tokens\"].append(token)\n",
    "        current_suborder[\"labels\"].append(label)\n",
    "\n",
    "        # Update flags for size and style\n",
    "        if label == 2:  # SIZE label\n",
    "            current_suborder[\"has_size\"] = True\n",
    "        elif label == 3:  # STYLE label\n",
    "            current_suborder[\"has_style\"] = True\n",
    "        elif label == 1:  # NUMBER label\n",
    "            current_suborder[\"has_number\"] = True\n",
    "        elif label == 8:  # DRINKTYPE label\n",
    "            current_suborder[\"has_drinktype\"] = True\n",
    "        elif label == 10:  # VOLUME label\n",
    "            current_suborder[\"has_volume\"] = True\n",
    "        elif label == 9:  # CONTAINERTYPE label\n",
    "            current_suborder[\"has_containertype\"] = True\n",
    "    \n",
    "    # Append the last suborder\n",
    "    if current_suborder[\"tokens\"]:\n",
    "        suborders.append(current_suborder)\n",
    "    \n",
    "    # Step 2: Add default Number if no Number found\n",
    "    for suborder in suborders:\n",
    "        if not suborder[\"has_number\"]:\n",
    "            suborder[\"tokens\"].insert(0, \"one\")\n",
    "            suborder[\"labels\"].insert(0, 1)  # Number label\n",
    "    \n",
    "    # Step 3: Classify suborders\n",
    "    order_details = []\n",
    "    for suborder in suborders:\n",
    "        tokens = suborder[\"tokens\"]\n",
    "        labels = suborder[\"labels\"]\n",
    "        # Determine order type\n",
    "        if any(label in [8, 9, 10] for label in labels):  # Drink order\n",
    "            order_type = \"DRINKORDER\"\n",
    "        elif any(label in [3, 4, 5, 6, 7] for label in labels):  # Pizza order\n",
    "            order_type = \"PIZZAORDER\"\n",
    "        else:\n",
    "            continue  # Skip invalid suborders\n",
    "        \n",
    "        suborder_details = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            token = tokens[i]\n",
    "            label = labels[i]\n",
    "            if label == 5:  # Quantity\n",
    "                if i != len(tokens) - 1:\n",
    "                    suborder_details.append(\n",
    "                        f\"(COMPLEX_TOPPING (QUANTITY {token.upper()}) (TOPPING {tokens[i + 1].upper()}))\"\n",
    "                    )\n",
    "                    i += 1\n",
    "                else:\n",
    "                    suborder_details.append(f\"({label_mapping[4]} {token.upper()})\")\n",
    "            elif label in [6, 7]:  # NOT cases (NOT_TOPPING, NOT_STYLE)\n",
    "                new_label = label_mapping[label][4:]  # Remove \"NOT_\" from the label string\n",
    "                suborder_details.append(\n",
    "                    f\"(NOT ({new_label} {token.upper()}))\"\n",
    "                )\n",
    "            elif label != 0:\n",
    "                # General case for non-zero labels\n",
    "                suborder_details.append(f\"({label_mapping[label]} {token.upper()})\")\n",
    "            i += 1\n",
    "        \n",
    "        order_details.append(f\"({order_type} {' '.join(suborder_details)})\")\n",
    "    \n",
    "    # Step 4: Combine into final order\n",
    "    return f\"(ORDER {' '.join(order_details)})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_to_dict(s):\n",
    "    def parse(tokens):\n",
    "        token = tokens.pop(0)\n",
    "        if token == '(':\n",
    "            key = tokens.pop(0)\n",
    "            nested = {}\n",
    "            while len(tokens) > 0 and tokens[0] != ')':\n",
    "                if tokens[0] == '(':\n",
    "                    temp_dict=parse(tokens)\n",
    "                    for k,v in temp_dict.items():\n",
    "                        if k in nested:\n",
    "                            nested[k].append(v)\n",
    "                        elif k == 'DRINKORDER' or k == 'PIZZAORDER' or k =='TOPPING' or k =='NOT' or k =='COMPLEX_TOPPING' or k == 'STYLE':\n",
    "                            nested[k]=[v]\n",
    "                        else:\n",
    "                            nested[k] = v\n",
    "                else:\n",
    "                    sub_key = tokens.pop(0)\n",
    "                    if tokens[0] == ')':\n",
    "                        tokens.pop(0)\n",
    "                        return {key: sub_key}\n",
    "            if len(tokens) > 0:\n",
    "                tokens.pop(0)  \n",
    "            return {key: nested}\n",
    "        elif token == ')':\n",
    "            return {}\n",
    "\n",
    "    # Tokenize the string\n",
    "    tokens = re.findall(r'\\(|\\)|\\w+', s)\n",
    "    return parse(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_toppings(order_json):\n",
    "\n",
    "    order = json.loads(order_json)\n",
    "    processed_order = {\"ORDER\": {\"PIZZAORDER\": [], \"DRINKORDER\": []}}\n",
    "    \n",
    "    for pizza in order[\"ORDER\"].get(\"PIZZAORDER\", []):\n",
    "        toppings = pizza.get(\"TOPPING\", [])\n",
    "        complex_toppings = pizza.get(\"COMPLEX_TOPPING\", [])\n",
    "        excluded = pizza.get(\"NOT\", {})\n",
    "        excluded_toppings = []\n",
    "        tops=[]\n",
    "        styles=[]\n",
    "        excluded_styles=[]\n",
    "        all_styles=[]\n",
    "        for exc in excluded:\n",
    "            tops.append(exc.get(\"TOPPING\", []))\n",
    "            if exc.get(\"STYLE\", False):\n",
    "                excluded_styles.append(exc.get(\"STYLE\", {})[0])\n",
    "        styles=pizza.get(\"STYLE\", [])         \n",
    "        for top in tops:\n",
    "            for i in range(len(top)):\n",
    "                excluded_toppings.append(top[i])\n",
    "            \n",
    "        if \"NOT\" in pizza:\n",
    "            del pizza[\"NOT\"]\n",
    "        if \"STYLE\" in pizza: \n",
    "            del pizza[\"STYLE\"]\n",
    "        if \"TOPPING\" in pizza:\n",
    "            del pizza[\"TOPPING\"]\n",
    "        if \"COMPLEX_TOPPING\" in pizza:\n",
    "            del pizza[\"COMPLEX_TOPPING\"]\n",
    "        all_toppings = []\n",
    "        \n",
    "        for topping in toppings:\n",
    "            all_toppings.append({\n",
    "                \"NOT\": False,\n",
    "                \"Quantity\": None,  \n",
    "                \"Topping\": topping\n",
    "            })\n",
    "        for style in styles:\n",
    "            all_styles.append({\n",
    "                \"NOT\": False,\n",
    "                \"TYPE\": style\n",
    "            })\n",
    "        for style in excluded_styles:\n",
    "            all_styles.append({\n",
    "                \"NOT\": True,\n",
    "                \"TYPE\": style\n",
    "            })\n",
    "        \n",
    "        for topping in excluded_toppings:\n",
    "            all_toppings.append({\n",
    "                \"NOT\": True,\n",
    "                \"Quantity\": None,  \n",
    "                \"Topping\": topping\n",
    "            })\n",
    "        for complex_topping in complex_toppings:\n",
    "            complex_quantity = complex_topping.get(\"QUANTITY\", None)\n",
    "            complex_topping_list = complex_topping.get(\"TOPPING\", [])\n",
    "            for topping in complex_topping_list:\n",
    "                all_toppings.append({\n",
    "                    \"NOT\": False,\n",
    "                    \"Quantity\": complex_quantity,\n",
    "                    \"Topping\": topping\n",
    "                })\n",
    "        for drink in order[\"ORDER\"].get(\"DRINKORDER\", []):\n",
    "            processed_order[\"ORDER\"][\"DRINKORDER\"].append(drink)\n",
    "        # Add the pizza order with updated AllTopping\n",
    "        new_pizza_order = {**pizza, \"AllTopping\": all_toppings, \"STYLE\": all_styles}\n",
    "        processed_order[\"ORDER\"][\"PIZZAORDER\"].append(new_pizza_order)\n",
    "    return json.dumps(processed_order,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_order(order_json):\n",
    "    # Parse the input JSON\n",
    "    order = json.loads(order_json)\n",
    "    processed_order = {\"ORDER\": {\"PIZZAORDER\": [], \"DRINKORDER\": []}}\n",
    "    \n",
    "    for pizza in order[\"ORDER\"].get(\"PIZZAORDER\", []):\n",
    "        number = pizza.get(\"NUMBER\", None)\n",
    "        size = pizza.get(\"SIZE\", None)\n",
    "        style = pizza.get(\"STYLE\", None) \n",
    "        type_ = pizza.get(\"TYPE\", None)  \n",
    "        toppings = pizza.get(\"AllTopping\", [])\n",
    "        \n",
    "        excluded_toppings = pizza.get(\"NOT\", {}).get(\"TOPPING\", [])\n",
    "        toppings = [topping for topping in toppings if topping not in excluded_toppings]\n",
    "        \n",
    "        processed_order[\"ORDER\"][\"PIZZAORDER\"].append({\n",
    "            \"NUMBER\": number,\n",
    "            \"SIZE\": size,\n",
    "            \"STYLE\": style,\n",
    "            \"ALLTOPPING\": toppings\n",
    "        })\n",
    "    for drink in order[\"ORDER\"].get(\"DRINKORDER\", []):\n",
    "        number = drink.get(\"NUMBER\", None)\n",
    "        size = drink.get(\"SIZE\", None)\n",
    "        drink_type = drink.get(\"DRINKTYPE\", None)\n",
    "        container_type = drink.get(\"CONTAINERTYPE\", None)\n",
    "\n",
    "        processed_order[\"ORDER\"][\"DRINKORDER\"].append({\n",
    "            \"NUMBER\": number,\n",
    "            \"SIZE\": size,\n",
    "            \"DRINKTYPE\": drink_type,\n",
    "            \"CONTAINERTYPE\": container_type\n",
    "        })\n",
    "    \n",
    "    return json.dumps(processed_order, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json(data):\n",
    "    if isinstance(data, dict):\n",
    "        # Process dictionary keys and values recursively\n",
    "        return {key: process_json(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        # Process each element in the list recursively\n",
    "        return [process_json(item) for item in data]\n",
    "    elif isinstance(data, str):\n",
    "        # Convert string to lowercase and handle underscores\n",
    "        return data.lower().replace('_', ' ')\n",
    "    else:\n",
    "        # Return data as-is if it's not a string, list, or dictionary\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, sequence in enumerate(pred_test):\n",
    "    combined_tokens, combined_labels = combine_tokens(order_tokens_2[i], sequence)\n",
    "    EXR_Format = Get_EXR_Format(combined_tokens, combined_labels)\n",
    "    result = parse_to_dict(EXR_Format)\n",
    "    json_string = json.dumps(result)\n",
    "    processed_toppings = process_toppings(json_string)\n",
    "    Json_Format=  process_order(processed_toppings)\n",
    "    final_Json=process_json(json.loads(Json_Format))\n",
    "    print(f\"test {i}\",final_Json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
